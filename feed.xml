<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="https://kfirlevari.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kfirlevari.github.io/" rel="alternate" type="text/html" /><updated>2022-01-11T13:06:04-08:00</updated><id>https://kfirlevari.github.io/feed.xml</id><title type="html">Kfir Lev-Ari</title><subtitle>Kfir Lev-Ari's Homepage</subtitle><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><entry><title type="html">Distributed Deep Neural Network (DNN) Training for Practitioners</title><link href="https://kfirlevari.github.io/DNN-Training/" rel="alternate" type="text/html" title="Distributed Deep Neural Network (DNN) Training for Practitioners" /><published>2022-01-11T00:00:00-08:00</published><updated>2022-01-11T00:00:00-08:00</updated><id>https://kfirlevari.github.io/DNN-Training</id><content type="html" xml:base="https://kfirlevari.github.io/DNN-Training/">&lt;p&gt;I love both distributed systems and machine learning. However, when I started grad school, I wasn’t sure which one to choose. In my first semester, the ML 101 course wasn’t taught while  parallel computing 101 did, and so I decided based on availability :smile:&lt;/p&gt;

&lt;p&gt;This topic of distributed training fascinates me because it combines essential aspects of both domains.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this survey, we cover strategies and techniques related to distributed DNN training.
The goal is to explore/highlight aspects of state-of-the-art approaches and make them more accessible to practitioners.&lt;/p&gt;

&lt;p&gt;Some basic knowledge in deep neural networks is required (if you’re missing it, you can see Section 2 and Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and/or Section 2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The structure of this survey is as follows -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Section &lt;a href=&quot;#dimensions-of-concurrency&quot;&gt;Dimensions of Concurrency&lt;/a&gt; gives an overview of the different dimensions of the parallel &amp;amp; distributed training problem.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#network-level-optimizations&quot;&gt;Network Level Optimizations&lt;/a&gt; focuses on the network aspect of training and related optimizations.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#hardware-level-optimizations&quot;&gt;Hardware Level Optimizations&lt;/a&gt; discusses CPU, GPU, and switch optimizations.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#fault-tolerance&quot;&gt;Fault Tolerance&lt;/a&gt; touches on a few techniques used by distributed training systems to deal with failures.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#from-ideas-to-practice&quot;&gt;From Ideas to Practice&lt;/a&gt; maps the ideas presented in this survey to available tools.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#building-a-training-service---related-topics&quot;&gt;Building a Training Service - Related Topics&lt;/a&gt; notes more directions to consider when building a training service&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dimensions-of-concurrency&quot;&gt;Dimensions of Concurrency&lt;/h2&gt;

&lt;p&gt;In this section, we give a short overview of the three primary computation levels found in DNN training that can be improved with concurrency - we start with the basic unit of a single DNN operator (Section &lt;a href=&quot;#single-dnn-operator-computation&quot;&gt;Single DNN Operator Computation&lt;/a&gt;), then look at a bigger picture that consists of the DNN structure itself (Section &lt;a href=&quot;#dnn-model-computation&quot;&gt;DNN Model Computation&lt;/a&gt;), and finish with the highest level which is the overall training flow (Section &lt;a href=&quot;#handling-multiple-copies-of-a-model&quot;&gt;Handling Multiple Copies of a Model&lt;/a&gt;).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure1&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 1. Dimensions of Concurrency in DNN Training. On the left - single operator computation, in the middle - single model evaluation (via forward evaluation &amp;amp; back propagation), and on the right - maintaining multiple states of the same model.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;single-dnn-operator-computation&quot;&gt;Single DNN Operator Computation&lt;/h3&gt;

&lt;p&gt;The main DNN operator types are - 1. Activation, 2. Fully Connected, 3. Convolution, 4. Pooling, and 5. Batch Normalization. 
Generally, the opportunities for parallelization in their computation are rooted in the locality of the operations that are done as part of each operator and the dimensionality of the data. The data dimensions are - 1. the minibatch size, 2. the number of channels/features/neurons, 3. the single sample dimensions (i.e., height, width), and 4. the convolution kernel dimensions (in convolution operator).&lt;/p&gt;

&lt;p&gt;At their core, nearly all operators are computed via matrix multiplication, and many optimizations in the literature focus on that. For more info, see Section 5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;dnn-model-computation&quot;&gt;DNN Model Computation&lt;/h3&gt;

&lt;p&gt;DNN models are created using SGD (see &lt;a href=&quot;#alg1&quot;&gt;Alg. 1.&lt;/a&gt;). This calculation can be partitioned among multiple processors due to the following dimensions: 1. the minibatch granularity, 2. the breath of the layers, and 3. the depth of the DNN.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;alg1&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Alg1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Alg. 1. Minibatch Stochastic Gradient Descent (SGD) with Backpropagation.
Algorithm adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;There are three prominent schemes for partitioning as depicted in &lt;a href=&quot;#Figure2&quot;&gt;Figure 2.&lt;/a&gt;: 1. partition by input samples (data parallelism), 2. by network structure (model parallelism), and 3. by layer (pipelining). In addition, there’s a hybrid approach that combines multiple schemes to overcome the drawbacks of each method.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure2&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure2.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 2. DNN Parallelism Schemes.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h4 id=&quot;data-parallelism&quot;&gt;Data Parallelism&lt;/h4&gt;

&lt;p&gt;In this approach, minibatches are partitioned among multiple computational resources. 
Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simple to reason about.&lt;/li&gt;
  &lt;li&gt;All operators except Batch Normalization operate on a single sample. Therefore in this approach, we can run forward evaluation and backpropagation almost independently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This approach requires each resource to maintain the whole model, as each sample might modify any of the model’s parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info, see Section 6.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;model-parallelism&quot;&gt;Model Parallelism&lt;/h4&gt;

&lt;p&gt;In this approach, different parts of the DNN are computed on different computational resources. 
Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Requires less memory from each computation resource (no need to store the whole model locally)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The minibatches are copied to all computation resources.&lt;/li&gt;
  &lt;li&gt;Communication intensive (e.g., handling a fully connected layer) significantly affects performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generally, effectively splitting models isn’t easy, and stalling due to communication overhead or synchronizations may occur. This means that increasing the degree of model parallelism doesn’t necessarily lead to training speedup (as in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/icml/MirhoseiniPLSLZ17&quot;&gt;[3]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For more info, see Section 6.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;layer-pipelining&quot;&gt;Layer Pipelining&lt;/h4&gt;

&lt;p&gt;Pipelining can refer to :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Overlapping computations of layers - overlapping the steps of forward evaluation, backpropagation, and weight updates.
    &lt;ul&gt;
      &lt;li&gt;Pros: mitigates processor idle time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Layer partitioning - partitioning the DNN according to its depth to different computational resources
    &lt;ul&gt;
      &lt;li&gt;Pros: 1. no need to store the whole model locally, 2. fixed number of communication points between processors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons of pipelining -&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample arrival rate affects system utilization, and 2. latency proportional to the number of processors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more info, see Section 6.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;handling-multiple-copies-of-a-model&quot;&gt;Handling Multiple Copies of a Model&lt;/h3&gt;

&lt;p&gt;In a distributed environment, there are multiple instances of SGD running independently, and therefore there are multiple copies of the model. The different distributed training algorithms can be categories by the way they address the following domains: 1. &lt;a href=&quot;#model-consistency&quot;&gt;Model Consistency&lt;/a&gt;, 2. &lt;a href=&quot;#centralization&quot;&gt;Centralization&lt;/a&gt;, 3.&lt;a href=&quot;#parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/a&gt;, 4. &lt;a href=&quot;#optimization-algorithms&quot;&gt;Optimization Algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;model-consistency&quot;&gt;Model Consistency&lt;/h4&gt;

&lt;p&gt;Consistency is essentially a tradeoff between the potential loss in model quality or convergence speed (when training instances use a stale model) and the synchronization costs (of updating instances model).&lt;/p&gt;

&lt;p&gt;If we think about consistency as an axis, then that axis describes the possible states of the model as observed by the different SGD instances before running the SGD method. We have consistent model algorithms in which everyone observes the most updated model on one end. On the other end, we have inconsistent model algorithms, in which there is no guarantee on the staleness of the model that each participant holds. In between, some algorithms bound the staleness as a compromise between consistency and inconsistency.&lt;/p&gt;

&lt;p&gt;Consistent model algorithms - Pros: simple to reason about. Cons: costly synchronization that hurts scaling.&lt;/p&gt;

&lt;p&gt;Inconsistent model algorithms - Pros: no extra synchronization is needed. Participants train almost independently. 
Cons: it doesn’t converge in all cases.&lt;/p&gt;

&lt;p&gt;Compromise approach - Pros: 1. pay for synchronization only after a bound is reached; 2. keeping lagging participants in check, and allows to add/remove on-the-fly participants;&lt;/p&gt;

&lt;p&gt;It seems like the consistent model algorithms are used nowadays for up to 35-50 participants, after which the added cost of communication breaks the linear scalability. Then, for higher participant numbers, the non-consistent model algorithms are used.&lt;/p&gt;

&lt;p&gt;There’s another dimension of model consolidation for inconsistent model algorithms that determines how the algorithm combines the resulting models of the different SGC instances - either post-training or several times during training.
A common post-training technique is ensemble learning (i.e., keeping an ensemble of models); other techniques are knowledge distillation (keeping an ensemble of smaller models that are trained based on a bigger one) and model averaging (that can also be used during training).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure3&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure3.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 3. Model consistency levels.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For more info, see Section 7.1 and Section 7.4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;centralization&quot;&gt;Centralization&lt;/h4&gt;

&lt;p&gt;Centralization range between the extreme of a single participant that is responsible for the collection, update, and distribution of the model (denoted by parameter server PS) to the other extreme of a completely decentralized network at which all participants share the responsibility for the model equally (in this case, the participants would rely on allreduce operations to communicate model updates among themselves).&lt;/p&gt;

&lt;p&gt;The selected design choice should consider the network characteristics (topology, bandwidth, and latency) and the required level of fault tolerance.
Note that PS consists of a single node on the extreme, but it’s possible to have an ensemble of participants that act together as a PS.&lt;/p&gt;

&lt;p&gt;The tradeoff between using either distribution scheme can be modeled by the communication cost per global update.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Centralize scheme&lt;/em&gt; -&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;PS can keep track of a “global view” of training&lt;/li&gt;
  &lt;li&gt;Averaging the gradients at one location&lt;/li&gt;
  &lt;li&gt;Enabling asynchronous operation mode of the training participants&lt;/li&gt;
  &lt;li&gt;Participants can communicate less information by performing some of the computations on the PS&lt;/li&gt;
  &lt;li&gt;Increases fault tolerance by dynamic spin-up and removal of nodes during training&lt;/li&gt;
  &lt;li&gt;enable handling heterogeneity, both in training agents and in network settings (e.g., latency, x-dc)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Requires each training participant to send and receive information from a specific subset of nodes (PS nodes) - unbalanced network usage pattern, e.g., a bottleneck on the PS with the increase of participants (in some cases, can be resolved using model parallelism and pipelining on the PS side).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the PS approach to scale, there’s a need to optimize software and hardware configurations and awareness of underlying physical network topology.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One example to such optimization (at rack scale) is  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/LuoNCPK18&quot;&gt;[4]&lt;/a&gt; - they show:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Analysis of three bottlenecks due to (1) network bandwidth, (2) software framework inefficiencies, and (3) suboptimal deployment.&lt;/li&gt;
      &lt;li&gt;Design &amp;amp; evaluate a rack-scale PS that mitigates the above issues&lt;/li&gt;
      &lt;li&gt;Design of an HW component that their PS is using and is responsible for rack-level and cross-rack gradient reductions&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another example, is a PS algorithm that uses Stale-Sync SGD (as described in Section &lt;a href=&quot;#model-consistency&quot;&gt;Model Consistency&lt;/a&gt;) - this paper &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/HoCCLKGGGX13&quot;&gt;[5]&lt;/a&gt; shows an example of such an implementation, evaluation, and proof of convergence.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Decentralize scheme&lt;/em&gt; -&lt;/p&gt;

&lt;p&gt;Pros: load balancing can be achieved using asynchronous training&lt;/p&gt;

&lt;p&gt;Cons: doesn’t scale well (e.g., see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/abs-1902-06855&quot;&gt;[6]&lt;/a&gt; for scalability analysis + suggested optimizations to mitigate the issues).&lt;/p&gt;

&lt;p&gt;Examples of optimizations for improving the scalability of this approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A ring-based Allreduce that was proven to be optimal in terms of bandwidth (see proof here &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/PatarasukY09&quot;&gt;[7]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;A decentralized version that is hierarchical (see Hierarchical All-reduce in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/abs-1807-11205&quot;&gt;[8]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;An adaptive algorithm that changes the number of partitions to correspond to the number of participants so that the network bandwidth remains constant (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/apsys/WangZGZ14&quot;&gt;[9]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Tunable partitioning of gradient updates that enable constant network bandwidth usage (i.e., the bandwidth remains unaffected by the number of participants). For more info, see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/WatcharapichatM16&quot;&gt;[10]&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Note that in low bandwidth or high latency networks, it might be that the decentralized approach behaves better than PS since it’s able to avoid the communication traffic jam (e.g., see two key observations on page 8 of &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/LianZZHZL17&quot;&gt;[11]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info on centralization, see Section 7.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/h4&gt;

&lt;p&gt;Compression relates to techniques that reduce message size. Generally, there are two compression levels -&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Compressing the model parameters/gradients with efficient data representations - as done via quantization (mapping continuous information into buckets that represent sets of values)&lt;/li&gt;
  &lt;li&gt;Avoid sending unnecessary information altogether (e.g., sending only relevant parts of sparse gradients), which results in communication of sparse data structures.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that some techniques are harder to implement in a decentralized scheme or even unique to the PS scheme (e.g., decreasing message size by sending activations and errors instead of parameters, relying on the PS computation capabilities to fill the missing computation steps).&lt;/p&gt;

&lt;p&gt;In addition, the computational cost of compression is non-negligible, and in some cases, outweighs the communication-reduction benefits (see &lt;a class=&quot;citation&quot; href=&quot;#10754/662495&quot;&gt;[13]&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;For more info, see Section 7.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and 3.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;optimization-algorithms&quot;&gt;Optimization Algorithms&lt;/h4&gt;

&lt;p&gt;Multiple optimizations aim to improve the training process but do not necessarily relate to the distributed nature of the computation (still, we highlight that distributed training algorithms might leverage these optimizations).&lt;/p&gt;

&lt;p&gt;E.g., for improving the parameter search, an algorithm might use a technique that reduces the variance of SGD (variance caused by the random sampling).&lt;/p&gt;

&lt;p&gt;Another example is an algorithm that tunes its hyper-parameters during execution or an algorithm that modifies its architecture (e.g., change the structure of layers).&lt;/p&gt;

&lt;p&gt;For more info, see Section 7.5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-level-optimizations&quot;&gt;Network Level Optimizations&lt;/h2&gt;

&lt;p&gt;Given the cluster size and compute capabilities growth, communication overheads became the main bottleneck of distributed training.&lt;/p&gt;

&lt;p&gt;The goal of this section is to focus on strategies for reducing it. 
In &lt;a href=&quot;#Figure4&quot;&gt;Figure 4.&lt;/a&gt;, we see the different optimization levels and domains that we cover in the following subsections (each in a separate sub-section).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure4&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure4.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 4. Summary of communication optimization levels. The top level refers to communication reduction and scheduling, and the lower level to network traffic.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;communication-rounds&quot;&gt;Communication Rounds&lt;/h3&gt;

&lt;p&gt;Training via minibatch SGD consists of multiple epochs (&lt;em&gt;epoch&lt;/em&gt; is an entire pass over the dataset) and iterations (iteration is one SGD round w/ one minibatch, see &lt;a href=&quot;#alg1&quot;&gt;Alg. 1.&lt;/a&gt;). In distributed training algorithms, data is often exchanged at the end of an iteration (e.g., sending gradients, sending model parameters, etc…). Therefore reducing the number of iterations means cutting down the communication rounds.&lt;/p&gt;

&lt;p&gt;The two straightforward ways to reduce the total number of iterations - 1. increasing the batch size and 2. reducing the frequency of exchanging gradients and/or parameters.&lt;/p&gt;

&lt;h4 id=&quot;batch-size&quot;&gt;Batch Size&lt;/h4&gt;

&lt;p&gt;The batch size is a hyper-parameter that controls the amount of data read before each iteration.&lt;/p&gt;

&lt;p&gt;Selecting a too small mini-batch size (see Section A in &lt;a href=&quot;#Figure5&quot;&gt;Figure 5.&lt;/a&gt;) can result in both:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;reduced utilization of the hardware during training (as the max concurrency level isn’t reached), as seen by the Performance line; and&lt;/li&gt;
  &lt;li&gt;statistical inaccuracy due to the variances introduced into the gradients (i.e., the randomly selected samples might not represent the dataset accurately) as seen by the Validation Error line.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While selecting a large batch size directly answers the concurrency aspect (see Performance line, Section C in &lt;a href=&quot;#Figure5&quot;&gt;Figure 5.&lt;/a&gt;), it might hurt the model accuracy (see Validation Error line) by hurting the optimizations used during training. This can be addressed by several techniques, such as adjusting the learning rate to be a function of the minibatch size, and more (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/GoyalDGNWKTJH17&quot;&gt;[14]&lt;/a&gt;, and Section 3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;). Note that an earlier paper &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/iclr/KeskarMNST17&quot;&gt;[15]&lt;/a&gt; claimed that the reason for reduced model accuracy with big minibatch sizes is a convergence to a sharp minimum vs. flat minimum with smaller sizes.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure5&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure5.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 5. Performance and accuracy of minibatch SGD after a fixed number of epochs.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h4 id=&quot;periodic-communication&quot;&gt;Periodic Communication&lt;/h4&gt;

&lt;p&gt;As we mentioned before, our goal is to reduce the number of communication rounds, i.e., occurrences at which participants send/receive model parameters and/or gradients.&lt;/p&gt;

&lt;p&gt;The idea here is that instead of sending the parameters and gradients after each iteration (see (a) in &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;), the average of multiple iterations will be sent after those iterations (e.g., (b) in &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;). The extreme case is a one-shot training at which nothing is sent until all iterations are done (i.e., (c) in  &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The tradeoff of averaging is between communication (that we reduce) and accuracy (that we lose), yet studies showed multiple approaches for averaging that can reduce communication costs while keeping the desired accuracy levels (see Section 3.1.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure6&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure6.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 6. Different model averaging approached (from no averaging in (a) to the maximum averaging in (c)). A green block indicates computation and a yellow block corresponds to communication.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Another aspect of communication rounds is their typical bursty behavior - in some algorithms (e.g., synchronous approaches), participants send their updates around the same time, leading to unbalanced network usage (in time and locations). In addition, message prioritization can help in some cases in order to meet bandwidth limitations, e.g., depend on the model state (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/mlsys/HashemiJC19&quot;&gt;[16]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/WeiDQHCGGGX15&quot;&gt;[17]&lt;/a&gt;). Note that these algorithms aren’t implemented yet in open source DL frameworks.&lt;/p&gt;

&lt;h3 id=&quot;gradient-compression&quot;&gt;Gradient Compression&lt;/h3&gt;

&lt;p&gt;See Section &lt;a href=&quot;#parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overlap&quot;&gt;Overlap&lt;/h3&gt;

&lt;p&gt;The idea here is to hide the communication overhead by performing the transmission during computation.&lt;/p&gt;

&lt;p&gt;One example is performing transmission of a layer during the computations of its preceding layer. In case of high latency, there are optimizations such as sending merged gradient messages (of different layers) that can reduce the amount of data sent. In addition,&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot; Figure7&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure7.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 7. One possible case of hiding the communication time overhead behind the backpropagation. In this case, each layer gradients are transmitted as soon as they are ready.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Another example is performing transmission of the weights while the current iteration is still running (while balancing training speed and accuracy). For more details on this approach of hybrid SSGD and ASGD, see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/taco/XuDZXL20&quot;&gt;[18]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;logical-architecture&quot;&gt;Logical Architecture&lt;/h3&gt;

&lt;p&gt;For discussion about topology, see &lt;a href=&quot;#centralization&quot;&gt;Centralization&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;messaging-library&quot;&gt;Messaging Library&lt;/h3&gt;

&lt;p&gt;According to &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;, most of the parameter servers are using either gRPC or ZeroMQ (see here for a nice comparison  &lt;a class=&quot;citation&quot; href=&quot;#grpc-and-zeromq-comparison&quot;&gt;[19]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Many message-level libraries are implementing Allreduce algorithms efficiently, such as MPI, Gloo, Baidu Allreduce, Aluminum, and BlueConnect (the latter seems to be more efficient than Gloo and Baidu Allreduce).&lt;/p&gt;

&lt;p&gt;For more info, see Section 4.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;protocols&quot;&gt;Protocols&lt;/h3&gt;

&lt;p&gt;At this level, improving the communication can be done by reducing the cost of copying data among machines, by moving away from the expensive TCP/IP protocol over sockets (Figure (a) in &lt;a href=&quot;#Figure8&quot;&gt;Figure 8.&lt;/a&gt;, to IPoIB (internet over InfiniBand) or RDMA (Figures (b) and (c) in &lt;a href=&quot;#Figure8&quot;&gt;Figure 8.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;RDMA allows one machine to read/write directly from/to the memory of another device without going through the OS (middle layer in the figures). This enables high performance and low latency networking. Further optimizations exist, such as letting GPUs communicate directly over RDMA or doing an adaptive RDMA by dynamically adjusting gRPC message sizes.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure8&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure8.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 8. Comparison of TCP/IP Socket, IPoIB, and RDMA.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Experiments show that using RDMA or IPoIB can significantly speed up training. Some even show near-linear speedup with RDMA when scaling training on 100 GPUs.&lt;/p&gt;

&lt;p&gt;For more info, See Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hardware-level-optimizations&quot;&gt;Hardware Level Optimizations&lt;/h2&gt;

&lt;h3 id=&quot;gpu-vs-cpu&quot;&gt;GPU vs CPU&lt;/h3&gt;

&lt;p&gt;The performance benefits of GPU compared to CPU depend on many factors, such as whether the job is processing-bound or memory-bound, the efficiency of the implementation, and the hardware characteristics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/isca/LeeKCDKNSSCHSD10&quot;&gt;[20]&lt;/a&gt; Intel show how the actual gap between GPU and CPU can be only 2.5x in favor of GPU when using the proper optimizations and HW characteristics. In that paper, they cover the following items:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;CPU optimizations - multithreading, cache blocking, and reorganization of memory accesses for SIMD-ification.&lt;/li&gt;
      &lt;li&gt;GPU optimizations - minimizing global synchronization and using local shared buffers.&lt;/li&gt;
      &lt;li&gt;HW architecture guidelines - high compute flops and memory bandwidth, large caches, gather/scatter support, efficient synchronization and cache coherence, and fixed functional units.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some systems use CPUs and GPUs for training, and solely CPUs for inference - since GPUs are great for bandwidth but not for latency (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/hpca/HazelwoodBBCDDF18&quot;&gt;[21]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/pvldb/ZouJLGWX14&quot;&gt;[22]&lt;/a&gt;). 
  Other relies solely on CPUs for training due to the abundance of available resources (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/DeanCMCDLMRSTYN12&quot;&gt;[23]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt;)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;There are some interesting optimizations in these papers, e.g., in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt; they partition the DL model layers to fit L3 cache, which shows to improve performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-gpu-execution&quot;&gt;Multi-GPU Execution&lt;/h3&gt;

&lt;p&gt;Gaining performance from multi-GPU scaling isn’t trivial, and understanding the performance impact of GPUs interconnections is essential.&lt;/p&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/tpds/LiSCLLTB20&quot;&gt;[25]&lt;/a&gt; they perform an evaluation of latest GPU interconnections w.r.t. raw startup latency, sustainable uni/bi-directional bandwidth, network topology, communication efficiency, routing, and NUMA (non uniform memory access) effects, under two communication patterns - Peer-to-Peer, and Collective.&lt;/p&gt;

&lt;p&gt;They identified several items, e.g., an “anti-locality” pattern (in some NUMA scenarios) at which nearby memory access presents lower performance than remote memory access, and suggested that the root cause is in the unbalanced physical signal paths (e.g., in PCIe-switch chipsets they used).&lt;/p&gt;

&lt;h3 id=&quot;switch-optimizations&quot;&gt;Switch Optimizations&lt;/h3&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nsdi/SapioC0NKKKMPR21&quot;&gt;[26]&lt;/a&gt; they show how to ease network bottleneck by performing the aggregation part of training on programmable switches (they focus on a single-rack).&lt;/p&gt;

&lt;p&gt;Although these aggregations can be done on the PS side (when using the PS approach), there’s a cost of required computation resources and network bandwidth.&lt;/p&gt;

&lt;h3 id=&quot;cnn-optimizations&quot;&gt;CNN Optimizations&lt;/h3&gt;

&lt;h4 id=&quot;layers-fusion&quot;&gt;Layers Fusion&lt;/h4&gt;

&lt;p&gt;Layer fusion is a technique for accelerating CNN by &lt;em&gt;fusing&lt;/em&gt; the processing of multiple CNN layers. 
Generally, instead of waiting for each layer processing to complete (which results in off-chip memory consumption for the large intermediate states between layers), the input-data processing order is altered to leverage caching of intermediate states of adjacents layers (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/micro/AlwaniCFM16&quot;&gt;[27]&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&quot;flexible-buffering&quot;&gt;Flexible Buffering&lt;/h4&gt;

&lt;p&gt;The same set of weights is used in all image evaluations in CNN networks.&lt;/p&gt;

&lt;p&gt;Accelerators leverage this by fetching batches of images at a time into on-chip memory, bringing blocks of weights to the on-chip memory, and then reusing the same weight block over the batched images for layer evaluation.&lt;/p&gt;

&lt;p&gt;If we consider a given on-chip storage budget, we see a tradeoff between the batch size (i.e., the number of images) and the amount of data per image stored on-chip.&lt;/p&gt;

&lt;p&gt;This means that by increasing the batch size, we reduce the number of times we need to fetch weight blocks into memory (i.e., reducing the weights bandwidth) but pay by increasing the required bandwidth for transferring data (as we store less per image on-chip).&lt;/p&gt;

&lt;p&gt;In  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/fccm/ShenFM17&quot;&gt;[28]&lt;/a&gt; they propose a flexible accelerator design that sets the batch size after considering optimal memory bandwidth.&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other Optimizations&lt;/h3&gt;

&lt;p&gt;For a discussion about other type of hardware approaches (e.g., FPGAs, TPUs) see Section 3.1.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault Tolerance&lt;/h2&gt;

&lt;p&gt;What does it mean for an algorithm to be fault-tolerant? The answer depends on the type of failure, the number of failures, and many other dimensions.&lt;/p&gt;

&lt;p&gt;This topic is beyond the scope of our survey. Still, given the nature of distributed environments in which failures are inevitable (e.g., restarting a server as part of continuous integration), we will briefly overview a few distributed training systems and their approaches to dealing with failures.&lt;/p&gt;

&lt;p&gt;Note that it’s crucial to address failures when building a training service - given that the service time (i.e., training time) can be hours, days, or even weeks. In other words, IMHO, asking clients to re-train their model due to a faulty server is not the best design choice.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Project Adam (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt;):
    &lt;ul&gt;
      &lt;li&gt;Impact of slow machines:
        &lt;ul&gt;
          &lt;li&gt;Single sample processing speed when using partitioned model approach depends on the slowest participant. To mitigate this problem, they allowed threads on each participant to process multiple samples in parallel.&lt;/li&gt;
          &lt;li&gt;The slowest participant delays the end time of each epoch, as all training samples are required to compute model prediction error on the validation data set (which determines if another epoch is needed). They addressed this issue by ending epochs after a fraction of the samples (e.g., 75%) are completely processed and shuffling the samples order before each epoch to ensure coverage.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Each parameter shard (a 1MB block of parameters) has three copies stored on a different parameter server. A primary-backup scheme is used among those three servers (i.e., a primary that serves the parameters and two secondaries that maintain copies).&lt;/li&gt;
      &lt;li&gt;Clients communicate with a fault-tolerant coordination service to find the parameter server that holds their required parameter shard. This coordination service is responsible for sharding the model and for the assignments of PS servers (generally, it uses Paxos). A lease mechanism is used for detecting faulty servers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GeePS and Poseidon (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/eurosys/CuiZGGX16&quot;&gt;[29]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/usenix/ZhangZXDHLHWXX17&quot;&gt;[30]&lt;/a&gt;):
    &lt;ul&gt;
      &lt;li&gt;In these systems, the parameter server continuously checkpoints the current states in persistent storage.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;from-ideas-to-practice&quot;&gt;From Ideas to Practice&lt;/h2&gt;

&lt;p&gt;This section gives an overview of the leading frameworks and libraries available today that supports distributed training.&lt;/p&gt;

&lt;p&gt;In the following list, we mention for each framework its name, APIs, and supported algorithms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Chainer (&lt;a class=&quot;citation&quot; href=&quot;#Chainer&quot;&gt;[31]&lt;/a&gt;) | APIs: Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Decentralized only&lt;/li&gt;
      &lt;li&gt;Synchronous only&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DL4j (&lt;a class=&quot;citation&quot; href=&quot;#DL4J&quot;&gt;[32]&lt;/a&gt;) | APIs: Java | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Modified 1-bit gradient quantization by Strom supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Keras (&lt;a class=&quot;citation&quot; href=&quot;#Keras&quot;&gt;[33]&lt;/a&gt;) | APIs: DL4j, TensorFlow | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Model quantization supported&lt;/li&gt;
      &lt;li&gt;Higher-level concepts must be implemented in the DL framework that employs Keras&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MXNet (&lt;a class=&quot;citation&quot; href=&quot;#MXNet&quot;&gt;[34]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#MXNet-with-Horovod&quot;&gt;[35]&lt;/a&gt;) | APIs: C++, Go, Java-Script, Matlab, Python, Scala, Wolfram, Julia, Perl, R | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization supported&lt;/li&gt;
      &lt;li&gt;2-bit gradient quantization with error-feedback supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PyTorch (See Section &lt;a href=&quot;#pytorch&quot;&gt;PyTorch&lt;/a&gt;) | APIs: C++, Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization is supported in beta&lt;/li&gt;
      &lt;li&gt;Gradient quantization is supported in beta&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SINGA (&lt;a class=&quot;citation&quot; href=&quot;#Signa&quot;&gt;[36]&lt;/a&gt;) | APIs: C++, Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TensorFlow (&lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/AbadiBCCDDDGIIK16&quot;&gt;[37]&lt;/a&gt;, and see Section &lt;a href=&quot;#tensorflow--w-keras&quot;&gt;TensorFlow&lt;/a&gt;) | APIs: C++, Go, Java, Java-Script, Python, Swift | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization supported (with Keras)&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported (but on roadmap..)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We now focus on two frameworks - TensorFlow and PyTorch:&lt;/p&gt;

&lt;h3 id=&quot;tensorflow--w-keras&quot;&gt;TensorFlow ( w/ Keras)&lt;/h3&gt;

&lt;p&gt;TensorFlow natively supports distributed and parallel training (see &lt;a class=&quot;citation&quot; href=&quot;#Distributed-training-with-TensorFlow&quot;&gt;[38, 39, 40, 41, 42, 43]&lt;/a&gt;). In particular, it supports both model parallelism and data parallelism. In data parallelism, the centralized approach via parameter servers is supported, using either asynchronous or synchronous training. Trained models can be quantized using TensorFlow Lite (see &lt;a class=&quot;citation&quot; href=&quot;#Post-training-quantization&quot;&gt;[44]&lt;/a&gt;). Currently, there is no native support for gradient quantization &lt;a class=&quot;citation&quot; href=&quot;#Quantization-aware-training&quot;&gt;[45]&lt;/a&gt;, but this can be done via Keras &lt;a class=&quot;citation&quot; href=&quot;#Quantization-aware-training-in-Keras-example&quot;&gt;[46]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;PyTorch has native support for distributed, data-parallel training, as well as model-parallel training (see &lt;a class=&quot;citation&quot; href=&quot;#PYTORCH-distributed&quot;&gt;[47, 48]&lt;/a&gt;). For data-parallel training, PyTorch implements the decentralized architecture and supports synchronous and asynchronous training. PyTorch supports model quantization via the QNNPACK library (see &lt;a class=&quot;citation&quot; href=&quot;#Quantized&quot;&gt;[49]&lt;/a&gt;). Gradient quantization is supported (see &lt;a class=&quot;citation&quot; href=&quot;#Pytorch-QUANTIZATION&quot;&gt;[50]&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;building-a-training-service---related-topics&quot;&gt;Building a Training Service - Related Topics&lt;/h2&gt;

&lt;p&gt;Topics that weren’t covered in this survey but are related to building &amp;amp; maintaining a training service -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scheduling and Elasticity (see Section 3.4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Data Management (see Section 3.5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;epilogue&quot;&gt;Epilogue&lt;/h2&gt;

&lt;p&gt;I hope you found this summary helpful and that you feel that you know the challenges in distributed training.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/I-Know-DT.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;I tried to keep it short and mention just the essential parts required for developing an intuition about the topic.
If you think something is missing, incorrect, or just can be improved, please feel free to reach out. I’ll appreciate any feedback. Thanks!&lt;/p&gt;

&lt;h2 id=&quot;refrences&quot;&gt;Refrences&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;DBLP:journals/csur/Ben-NunH19&quot;&gt;Tal Ben-Nun and Torsten Hoefler. &lt;b&gt;Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency
               Analysis&lt;/b&gt;. &lt;i&gt;ACM Comput. Surv.&lt;/i&gt;, 52(4) :65:1–65:43, 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/csur/MayerJ20&quot;&gt;Ruben Mayer and Hans-Arno Jacobsen. &lt;b&gt;Scalable Deep Learning on Distributed Infrastructures: Challenges,
               Techniques, and Tools&lt;/b&gt;. &lt;i&gt;ACM Comput. Surv.&lt;/i&gt;, 53(1) :3:1–3:37, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/icml/MirhoseiniPLSLZ17&quot;&gt;Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. &lt;b&gt;Device Placement Optimization with Reinforcement Learning&lt;/b&gt;. In: Precup, D. and Teh, Y.W. (eds.) &lt;i&gt;Proceedings of the 34th International Conference on Machine Learning,
               ICML 2017, Sydney, NSW, Australia, 6-11 August 2017&lt;/i&gt;, pp. 2430–2439, PMLR, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/LuoNCPK18&quot;&gt;Liang Luo, Jacob Nelson, Luis Ceze, Amar Phanishayee, and Arvind Krishnamurthy. &lt;b&gt;Parameter Hub: a Rack-Scale Parameter Server for Distributed Deep
               Neural Network Training&lt;/b&gt;. In: &lt;i&gt;Proceedings of the ACM Symposium on Cloud Computing, SoCC 2018,
               Carlsbad, CA, USA, October 11-13, 2018&lt;/i&gt;, pp. 41–54, ACM, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/HoCCLKGGGX13&quot;&gt;Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Gregory R. Ganger, and Eric P. Xing. &lt;b&gt;More Effective Distributed ML via a Stale Synchronous Parallel Parameter
               Server&lt;/b&gt;. In: Burges, C.J.C., Bottou, L., Ghahramani, Z., and Weinberger, K.Q. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States&lt;/i&gt;, pp. 1223–1231 2013.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/abs-1902-06855&quot;&gt;Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, and Yonggang Wen. &lt;b&gt;Optimizing Network Performance for Distributed DNN Training on GPU
               Clusters: ImageNet/AlexNet Training in 1.5 Minutes&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1902.06855 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/jpdc/PatarasukY09&quot;&gt;Pitch Patarasuk and Xin Yuan. &lt;b&gt;Bandwidth optimal all-reduce algorithms for clusters of workstations&lt;/b&gt;. &lt;i&gt;J. Parallel Distributed Comput.&lt;/i&gt;, 69(2) :117–124, 2009.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/abs-1807-11205&quot;&gt;Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen, Guangxiao Hu, Shaohuai Shi, and Xiaowen Chu. &lt;b&gt;Highly Scalable Deep Learning Training System with Mixed-Precision:
               Training ImageNet in Four Minutes&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1807.11205 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/apsys/WangZGZ14&quot;&gt;Minjie Wang, Hucheng Zhou, Minyi Guo, and Zheng Zhang. &lt;b&gt;A scalable and topology configurable protocol for distributed parameter
               synchronization&lt;/b&gt;. In: &lt;i&gt;Asia-Pacific Workshop on Systems, APSys’14, Beijing, China, June 25-26,
               2014&lt;/i&gt;, pp. 13:1–13:7, ACM, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/WatcharapichatM16&quot;&gt;Pijika Watcharapichat, Victoria Lopez Morales, Raul Castro Fernandez, and Peter R. Pietzuch. &lt;b&gt;Ako: Decentralised Deep Learning with Partial Gradient Exchange&lt;/b&gt;. In: Aguilera, M.K., Cooper, B., and Diao, Y. (eds.) &lt;i&gt;Proceedings of the Seventh ACM Symposium on Cloud Computing, Santa
               Clara, CA, USA, October 5-7, 2016&lt;/i&gt;, pp. 84–97, ACM, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/LianZZHZL17&quot;&gt;Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. &lt;b&gt;Can Decentralized Algorithms Outperform Centralized Algorithms? A
               Case Study for Decentralized Parallel Stochastic Gradient Descent&lt;/b&gt;. In: Guyon, I., Luxburg, U. von, Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., and Garnett, R. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, USA&lt;/i&gt;, pp. 5330–5340 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/jpdc/OuyangDXX21&quot;&gt;Shuo Ouyang, Dezun Dong, Yemao Xu, and Liquan Xiao. &lt;b&gt;Communication optimization strategies for distributed deep neural
               network training: A survey&lt;/b&gt;. &lt;i&gt;J. Parallel Distributed Comput.&lt;/i&gt;, 149 :52–65, 2021.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;10754/662495&quot;&gt;Hang Xu, Chen-Yu Ho, Ahmed M. Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos Karatsenidis, Marco Canini, and Panos Kalnis. &lt;b&gt;Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation&lt;/b&gt;. 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/GoyalDGNWKTJH17&quot;&gt;Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. &lt;b&gt;Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1706.02677 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/iclr/KeskarMNST17&quot;&gt;Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. &lt;b&gt;On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima&lt;/b&gt;. In: &lt;i&gt;5th International Conference on Learning Representations, ICLR 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings&lt;/i&gt;, OpenReview.net, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/mlsys/HashemiJC19&quot;&gt;Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, and Roy H. Campbell. &lt;b&gt;TicTac: Accelerating Distributed Deep Learning with Communication
               Scheduling&lt;/b&gt;. In: Talwalkar, A., Smith, V., and Zaharia, M. (eds.) &lt;i&gt;Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford,
               CA, USA, March 31 - April 2, 2019&lt;/i&gt;, mlsys.org, 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/WeiDQHCGGGX15&quot;&gt;Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson, and Eric P. Xing. &lt;b&gt;Managed communication and consistency for fast data-parallel iterative
               analytics&lt;/b&gt;. In: Ghandeharizadeh, S., Barahmand, S., Balazinska, M., and Freedman, M.J. (eds.) &lt;i&gt;Proceedings of the Sixth ACM Symposium on Cloud Computing, SoCC
               2015, Kohala Coast, Hawaii, USA, August 27-29, 2015&lt;/i&gt;, pp. 381–394, ACM, 2015.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/taco/XuDZXL20&quot;&gt;Yemao Xu, Dezun Dong, Yawei Zhao, Weixia Xu, and Xiangke Liao. &lt;b&gt;OD-SGD: One-Step Delay Stochastic Gradient Descent for Distributed
               Training&lt;/b&gt;. &lt;i&gt;ACM Trans. Archit. Code Optim.&lt;/i&gt;, 17(4) :30:1–30:26, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grpc-and-zeromq-comparison&quot;&gt;&lt;b&gt;grpc and zeromq comparsion&lt;/b&gt;, https://stackoverflow.com/questions/39350681/grpc-and-zeromq-comparsion, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/isca/LeeKCDKNSSCHSD10&quot;&gt;Victor W. Lee, Changkyu Kim, Jatin Chhugani, Michael Deisher, Daehyun Kim, Anthony D. Nguyen, Nadathur Satish, Mikhail Smelyanskiy, Srinivas Chennupaty, Per Hammarlund, Ronak Singhal, and Pradeep Dubey. &lt;b&gt;Debunking the 100X GPU vs. CPU myth: an evaluation of throughput
               computing on CPU and GPU&lt;/b&gt;. In: Seznec, A., Weiser, U.C., and Ronen, R. (eds.) &lt;i&gt;37th International Symposium on Computer Architecture (ISCA 2010),
               June 19-23, 2010, Saint-Malo, France&lt;/i&gt;, pp. 451–460, ACM, 2010.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/hpca/HazelwoodBBCDDF18&quot;&gt;Kim M. Hazelwood, Sarah Bird, David M. Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, and Xiaodong Wang. &lt;b&gt;Applied Machine Learning at Facebook: A Datacenter Infrastructure
               Perspective&lt;/b&gt;. In: &lt;i&gt;IEEE International Symposium on High Performance Computer Architecture,
               HPCA 2018, Vienna, Austria, February 24-28, 2018&lt;/i&gt;, pp. 620–629, IEEE Computer Society, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/pvldb/ZouJLGWX14&quot;&gt;Yongqiang Zou, Xing Jin, Yi Li, Zhimao Guo, Eryu Wang, and Bin Xiao. &lt;b&gt;Mariana: Tencent Deep Learning Platform and its Applications&lt;/b&gt;. &lt;i&gt;Proc. VLDB Endow.&lt;/i&gt;, 7(13) :1772–1777, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/DeanCMCDLMRSTYN12&quot;&gt;Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. &lt;b&gt;Large Scale Distributed Deep Networks&lt;/b&gt;. In: Bartlett, P.L., Pereira, F.C.N., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 25: 26th Annual
               Conference on Neural Information Processing Systems 2012. Proceedings
               of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States&lt;/i&gt;, pp. 1232–1240 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;Trishul M. Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. &lt;b&gt;Project Adam: Building an Efficient and Scalable Deep Learning Training
               System&lt;/b&gt;. In: Flinn, J. and Levy, H. (eds.) &lt;i&gt;11th USENIX Symposium on Operating Systems Design and Implementation,
               OSDI ’14, Broomfield, CO, USA, October 6-8, 2014&lt;/i&gt;, pp. 571–582, USENIX Association, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/tpds/LiSCLLTB20&quot;&gt;Ang Li, Shuaiwen Leon Song, Jieyang Chen, Jiajia Li, Xu Liu, Nathan R. Tallent, and Kevin J. Barker. &lt;b&gt;Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch
               and GPUDirect&lt;/b&gt;. &lt;i&gt;IEEE Trans. Parallel Distributed Syst.&lt;/i&gt;, 31(1) :94–110, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nsdi/SapioC0NKKKMPR21&quot;&gt;Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, and Peter Richtárik. &lt;b&gt;Scaling Distributed Machine Learning with In-Network Aggregation&lt;/b&gt;. In: Mickens, J. and Teixeira, R. (eds.) &lt;i&gt;18th USENIX Symposium on Networked Systems Design and Implementation,
               NSDI 2021, April 12-14, 2021&lt;/i&gt;, pp. 785–808, USENIX Association, 2021.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/micro/AlwaniCFM16&quot;&gt;Manoj Alwani, Han Chen, Michael Ferdman, and Peter A. Milder. &lt;b&gt;Fused-layer CNN accelerators&lt;/b&gt;. In: &lt;i&gt;49th Annual IEEE/ACM International Symposium on Microarchitecture,
               MICRO 2016, Taipei, Taiwan, October 15-19, 2016&lt;/i&gt;, pp. 22:1–22:12, IEEE Computer Society, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/fccm/ShenFM17&quot;&gt;Yongming Shen, Michael Ferdman, and Peter A. Milder. &lt;b&gt;Escher: A CNN Accelerator with Flexible Buffering to Minimize
               Off-Chip Transfer&lt;/b&gt;. In: &lt;i&gt;25th IEEE Annual International Symposium on Field-Programmable Custom
               Computing Machines, FCCM 2017, Napa, CA, USA, April 30 - May 2,
               2017&lt;/i&gt;, pp. 93–100, IEEE Computer Society, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/eurosys/CuiZGGX16&quot;&gt;Henggang Cui, Hao Zhang, Gregory R. Ganger, Phillip B. Gibbons, and Eric P. Xing. &lt;b&gt;GeePS: scalable deep learning on distributed GPUs with a GPU-specialized
               parameter server&lt;/b&gt;. In: Cadar, C., Pietzuch, P.R., Keeton, K., and Rodrigues, R. (eds.) &lt;i&gt;Proceedings of the Eleventh European Conference on Computer Systems,
               EuroSys 2016, London, United Kingdom, April 18-21, 2016&lt;/i&gt;, pp. 4:1–4:16, ACM, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/usenix/ZhangZXDHLHWXX17&quot;&gt;Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie, and Eric P. Xing. &lt;b&gt;Poseidon: An Efficient Communication Architecture for Distributed
               Deep Learning on GPU Clusters&lt;/b&gt;. In: Silva, D.D. and Ford, B. (eds.) &lt;i&gt;2017 USENIX Annual Technical Conference, USENIX ATC 2017, Santa
               Clara, CA, USA, July 12-14, 2017&lt;/i&gt;, pp. 181–193, USENIX Association, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chainer&quot;&gt;&lt;b&gt;Chainer&lt;/b&gt;, https://docs.chainer.org/en/stable/chainermn/index.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DL4J&quot;&gt;&lt;b&gt;DL4J’s Distributed Training Implementations&lt;/b&gt;, https://deeplearning4j.konduit.ai/spark/tutorials/dl4j-on-spark-quickstart#dl4js-distributed-training-implementations, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Keras&quot;&gt;&lt;b&gt;Keras - Multi-GPU and distributed training&lt;/b&gt;, https://keras.io/guides/distributed_training/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;MXNet&quot;&gt;&lt;b&gt;MXNet&lt;/b&gt;, https://mxnet.apache.org/versions/1.8.0/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;MXNet-with-Horovod&quot;&gt;&lt;b&gt;Distributed Training using Apache MXNet with Horovod&lt;/b&gt;, https://medium.com/apache-mxnet/distributed-training-using-apache-mxnet-with-horovod-44f98bf0e7b7, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Signa&quot;&gt;&lt;b&gt;Signa - Distributed Training&lt;/b&gt;, https://singa.apache.org/docs/dist-train/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/osdi/AbadiBCCDDDGIIK16&quot;&gt;Abadi Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. &lt;b&gt;TensorFlow: A System for Large-Scale Machine Learning&lt;/b&gt;. In: Keeton, K. and Roscoe, T. (eds.) &lt;i&gt;12th USENIX Symposium on Operating Systems Design and Implementation,
               OSDI 2016, Savannah, GA, USA, November 2-4, 2016&lt;/i&gt;, pp. 265–283, USENIX Association, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Distributed-training-with-TensorFlow&quot;&gt;&lt;b&gt;Distributed training with TensorFlow&lt;/b&gt;, https://www.tensorflow.org/guide/distributed_training, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;TensorFlow-Optimize-further&quot;&gt;&lt;b&gt;TensorFlow - Optimize further&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/optimize_further, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Distributed-TensorFlow-training&quot;&gt;&lt;b&gt;Distributed TensorFlow training (Google I/O ’18)&lt;/b&gt;, https://www.youtube.com/watch?v=bRMGoPqsn20, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Inside-TensorFlow-tf.data-tf.distribute&quot;&gt;&lt;b&gt;Inside TensorFlow: tf.data + tf.distribute (Dev Summit 2019)&lt;/b&gt;, https://www.youtube.com/watch?v=ZnukSLKEw34, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Inside-TensorFlow-tf.distribute.Strategy&quot;&gt;&lt;b&gt;Inside TensorFlow: tf.distribute.Strategy&lt;/b&gt;, https://www.youtube.com/watch?v=jKV53r9-H14, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Scaling-TensorFlow-2-models-to-multi-worker-GPUs&quot;&gt;&lt;b&gt;Scaling TensorFlow 2 models to multi-worker GPUs (TF Dev Summit ‘20)&lt;/b&gt;, https://www.youtube.com/watch?v=6ovfZW8pepo, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Post-training-quantization&quot;&gt;&lt;b&gt;Post-training quantization&lt;/b&gt;, https://www.tensorflow.org/lite/performance/post_training_quantization, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantization-aware-training&quot;&gt;&lt;b&gt;Quantization aware training&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/quantization/training, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantization-aware-training-in-Keras-example&quot;&gt;&lt;b&gt;Quantization aware training in Keras example&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/quantization/training_example, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;PYTORCH-distributed&quot;&gt;&lt;b&gt;PYTORCH DISTRIBUTED OVERVIEW&lt;/b&gt;, https://pytorch.org/tutorials/beginner/dist_overview.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;TORCH.DISTRIBUTED&quot;&gt;&lt;b&gt;DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED&lt;/b&gt;, https://pytorch.org/docs/stable/distributed.html?highlight=distributed#basics, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantized&quot;&gt;&lt;b&gt;Quantized Neural Networks PACKage&lt;/b&gt;, https://github.com/pytorch/QNNPACK, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pytorch-QUANTIZATION&quot;&gt;&lt;b&gt;Pytorch QUANTIZATION&lt;/b&gt;, https://pytorch.org/docs/stable/quantization.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html">I love both distributed systems and machine learning. However, when I started grad school, I wasn’t sure which one to choose. In my first semester, the ML 101 course wasn’t taught while parallel computing 101 did, and so I decided based on availability :smile:</summary></entry><entry><title type="html">DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub.</title><link href="https://kfirlevari.github.io/CreatingAWebSite/" rel="alternate" type="text/html" title="DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub." /><published>2021-12-21T00:00:00-08:00</published><updated>2021-12-21T00:00:00-08:00</updated><id>https://kfirlevari.github.io/CreatingAWebSite</id><content type="html" xml:base="https://kfirlevari.github.io/CreatingAWebSite/">&lt;p&gt;In this post, I’ll share how I built this website,
i.e., one hosted in GitHub Pages and powered by Jekyll &amp;amp; Minimal Mistakes.&lt;/p&gt;

&lt;p&gt;I think that it can be done by anyone enthusiastic about learning new technologies (actually, a keyboard, a screen, and a connection to the internet are also required :smile:).&lt;/p&gt;

&lt;p&gt;I assume you have a basic familiarity with git. If you’re unsure how to work with git, you can start &lt;a href=&quot;https://docs.github.com/en/get-started/using-git/about-git&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And most important, questions and comments are welcome :wink:&lt;/p&gt;

&lt;h2 id=&quot;preparations&quot;&gt;Preparations&lt;/h2&gt;

&lt;h3 id=&quot;step-1-github-pages&quot;&gt;Step 1. GitHub Pages&lt;/h3&gt;

&lt;p&gt;Like I mentioned, the site is hosted in GitHub Pages. What does it mean? see &lt;a href=&quot;https://pages.github.com/&quot;&gt;here&lt;/a&gt;.
A good starting point is to follow the instructions in that link and create your website repository (at &amp;lt;your GitHub username&amp;gt;.github.io).&lt;/p&gt;

&lt;p&gt;Later you’ll place there the content of your website. But, for now, you should only be able to see the “Hello World” page there.&lt;/p&gt;

&lt;p&gt;Let’s also create a local clone of your website repository. E.g., to “/Repos/&amp;lt;your GitHub username&amp;gt;.github.io” directory.&lt;/p&gt;

&lt;h3 id=&quot;step-2-preparations-for-jekyll--minimal-mistakes&quot;&gt;Step 2. Preparations for Jekyll &amp;amp; Minimal Mistakes&lt;/h3&gt;

&lt;p&gt;What’s Jekyll? a quote from &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;“Jekyll is a static site generator. It takes text written in your favorite markup language and uses layouts to create a static website.”&lt;/p&gt;

&lt;p&gt;So basically, Jekyll can build a website from files that we write in a markup language. What markup language we’ll use? a simple one named &lt;a href=&quot;https://www.markdownguide.org/getting-started/&quot;&gt;Markdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To run Jekyll, we need to prepare our environment. First, you should install the requirements listed &lt;a href=&quot;https://jekyllrb.com/docs/installation/#requirements&quot;&gt;here&lt;/a&gt; (i.e., Ruby, RubyGems, GCC, and Make) if you’re not sure how then follow the installation guide that matches your operating system &lt;a href=&quot;https://jekyllrb.com/docs/installation/#guides&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What’s Minimal Mistakes? From &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/about/&quot;&gt;here&lt;/a&gt;: “A flexible two-column Jekyll theme”. We’ll see later how to import it.&lt;/p&gt;

&lt;h3 id=&quot;step-3-source-directory-preparations&quot;&gt;Step 3. Source Directory Preparations&lt;/h3&gt;

&lt;p&gt;Jekyll will transform a source directory into a website. To verify the changes that we do in the source directory while working on our website and be resilient to failures (so that we won’t lose the source of our website if our computer crashes), we’ll use online version control.&lt;/p&gt;

&lt;p&gt;It would help to create a new repository for the source directory in this step.
For example - you can create another repository in GitHub. This time, it can be private (unlike &amp;lt;your GitHub username&amp;gt;.github.io that needs to be public).&lt;/p&gt;

&lt;p&gt;After creating the source directory repository, clone it locally (e.g., to “/Repos/source” directory). Then, we populate it in the next step.&lt;/p&gt;

&lt;h3 id=&quot;step-4-importing-minimal-mistakes&quot;&gt;Step 4. Importing Minimal Mistakes&lt;/h3&gt;

&lt;p&gt;Our goal in this step is to have &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/structure/&quot;&gt;these files and folders&lt;/a&gt; in your local source directory.&lt;/p&gt;

&lt;p&gt;One way to get these files is by cloning &lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal Mistakes repository&lt;/a&gt; locally (e.g., to “Repos/minimal-mistakes”), and then manually copying the required files to your “/Repos/source” directory.
After this step, we won’t use “Repos/minimal-mistakes”, so you can delete it.&lt;/p&gt;

&lt;h3 id=&quot;step-5-populate-the-gemfile&quot;&gt;Step 5. Populate the GemFile&lt;/h3&gt;

&lt;p&gt;What is RubyGems (that we installed as part of &lt;a href=&quot;#step-2-preparations-for-jekyll--minimal-mistakes&quot;&gt;Step 2&lt;/a&gt;)? From &lt;a href=&quot;https://en.wikipedia.org/wiki/RubyGems&quot;&gt;here&lt;/a&gt;: “RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries (in a self-contained format called a \ “gem&quot;), a tool designed to manage the installation of gems easily, and a server for distributing them.”&lt;/p&gt;

&lt;p&gt;We will populate the GemFile in “/Repos/source” with the packages that our website needs.&lt;/p&gt;

&lt;p&gt;Here is the GemFile I’m using:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source &quot;https://rubygems.org&quot;

gem &quot;github-pages&quot;, group: :jekyll_plugins
gem &quot;jekyll-include-cache&quot;, group: :jekyll_plugins
gem 'jekyll-academicons-svg'
gem 'jemoji'
gem 'jekyll-scholar', group: :jekyll_plugins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;github-pages, and jekyll-include-cache: required for us to be able to run our theme in GitHub Pages, as explained &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/#remote-theme-method&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jekyll-academicons-svg: in order to use &lt;a href=&quot;https://jpswalsh.github.io/academicons/&quot;&gt;academicons&lt;/a&gt;. Plugin repo &lt;a href=&quot;https://github.com/sylvainmetayer/jekyll-academicons-svg&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jemoji: because one can’t express oneself online these days without emojis, or can we :thinking:? Plugin repo &lt;a href=&quot;https://github.com/jekyll/jemoji&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jekyll-scholar: gives you citation superpowers, as explained &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-6-install-jekyll-and-bundler&quot;&gt;Step 6. Install Jekyll and Bundler&lt;/h3&gt;

&lt;p&gt;To install Jekyll and &lt;a href=&quot;https://bundler.io/&quot;&gt;Bundler&lt;/a&gt; gems locally, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem install jekyll bundler&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We’ll use bundler to install gems and run Jekyll.&lt;/p&gt;

&lt;h2 id=&quot;build-your-homepage&quot;&gt;Build Your Homepage&lt;/h2&gt;

&lt;h3 id=&quot;step-1-set-repository-name&quot;&gt;Step 1. Set Repository Name&lt;/h3&gt;

&lt;p&gt;The config file “_config.yml” contains many knobs. See &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;here&lt;/a&gt; for the full list.&lt;/p&gt;

&lt;p&gt;You don’t have to update all of them right now, but you do need to set some of them -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fill your website repository name, i.e, repository : “&amp;lt;your GitHub username&amp;gt;.github.io”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add “github: [metadata]” at the end of your config file (based on &lt;a href=&quot;https://github.com/github/pages-gem/issues/399&quot;&gt;this&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comment out both “theme:” and “remote theme:” lines - we’re using a local copy of Minimal Mistakes, so we don’t need to import anything else.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make sure that you have ‘jekyll-include-cache’ in the plugins array. For example, here is my plugins array (I think you’ll have the first 5):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plugins:
  - jekyll-paginate
  - jekyll-sitemap
  - jekyll-gist
  - jekyll-feed
  - jekyll-include-cache
  - jemoji
  - jekyll-scholar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-sanity-check&quot;&gt;Step 2. Sanity Check&lt;/h3&gt;

&lt;p&gt;In your source directory, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and once all went well, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve --incremental --verbose&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You should see your new website at &lt;a href=&quot;http://localhost:4000&quot;&gt;http://localhost:4000&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that the optional –incremental flag enables you to test your website while changing it (but it’s experimental, see &lt;a href=&quot;https://jekyllrb.com/docs/configuration/incremental-regeneration/&quot;&gt;here&lt;/a&gt;), and the optional –verbose flag can help you debug issues you have in can Jekyll fails to build your site.&lt;/p&gt;

&lt;h3 id=&quot;step-3-design-your-website&quot;&gt;Step 3. Design Your Website&lt;/h3&gt;

&lt;p&gt;I’m no web designer, so I don’t know the right protocol here :smile:. But let me share a few things that I did and might help you, do them in any order you see fit:&lt;/p&gt;

&lt;h4 id=&quot;config-file&quot;&gt;Config file&lt;/h4&gt;

&lt;p&gt;Look closely at the configurations &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;here&lt;/a&gt;, and modify your _config.yml accordingly.&lt;/p&gt;

&lt;h4 id=&quot;navigation&quot;&gt;Navigation&lt;/h4&gt;

&lt;p&gt;In _data/navigation.yml there’s the main array. Based on it, the links at the top are generated. E.g., my main array is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;main:
  - title: &quot;Blog&quot;
    url: /blog/
  - title: &quot;Projects&quot;
    url: /projects/
  - title: &quot;Publications&quot;
    url: /publications/
  - title: &quot;Past Teaching&quot;
    url: /past-teaching/
  - title: &quot;Personal&quot;
    url: /personal/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;pages-posts-and-index&quot;&gt;Pages, Posts, and Index&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Create a new directory named “_pages” (see also &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/pages/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://jekyllrb.com/docs/pages/&quot;&gt;here&lt;/a&gt;).
You should add a .md file in /_pages for every link that you added in the main array.
E.g., I have /_pages/personal.md, that corresponds to the personal page.&lt;/li&gt;
  &lt;li&gt;If you want a blog, create a new directory named “_posts” (see also &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/posts/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://jekyllrb.com/docs/posts/&quot;&gt;here&lt;/a&gt;).
Follow the name format when creating new posts. Note that only posts in the past are visible by default in the blog.&lt;/li&gt;
  &lt;li&gt;Your front page is generated based on the index.html file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;front-matter-and-layouts&quot;&gt;Front Matter and Layouts&lt;/h4&gt;

&lt;p&gt;At the top of every page, there’s a YAML front matter block. Read about it &lt;a href=&quot;https://jekyllrb.com/docs/front-matter/&quot;&gt;here&lt;/a&gt;.
For example, the front matter of this page is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: single
title: &quot;DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub.&quot;
toc: true
toc_label: &quot;Steps&quot;
toc_icon: &quot;shoe-prints&quot;
toc_sticky: true
---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(“toc” stands for Table Of Content :wink:)&lt;/p&gt;

&lt;p&gt;You set the page’s layout by changing these values (e.g., the layout of this page is “single”).
For all Minimal Mistakes layouts, see &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/layouts/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We copied Minimal Mistakes source code to our project folder (our source directory). This means that you can modify every template that you want, just be sure that you know what you’re doing :smile:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E.g., the “single” layout file is found at _layouts/single.html.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;publications&quot;&gt;Publications&lt;/h4&gt;
&lt;p&gt;I followed the instructions in &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot;&gt;jekyll-scholar&lt;/a&gt; and &lt;a href=&quot;http://pascalpoizat.github.io/blog/posts/2016/02/01/jekyll-and-bibtex/&quot;&gt;pascalpoizat.github.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Generally, I did the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Created /assets/bibliography directory&lt;/li&gt;
  &lt;li&gt;Placed there a bib style file (*.csl). Chose your favorite, or modify one; there are too many out there. I used &lt;a href=&quot;https://github.com/pascalpoizat/pascalpoizat.github.io/blob/master/src/_bibliography/mystyle.csl&quot;&gt;this style&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Also placed a references.bib file with all the BibTeX citations.&lt;/li&gt;
  &lt;li&gt;In the config file, create this array:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scholar:
  source: /assets/bibliography
  style: /assets/bibliography/mystyle.csl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Created the following publications.md file in /_pages/ :&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
permalink: /publications/
title: &quot;Publications&quot;
layout: single
classes: wide
header:
  overlay_image: /assets/images/Publications.jpg
  overlay_filter: 0.2
  caption: &quot;[See more photos here](/personal/#photography)&quot;
---

{ % bibliography % }

---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the header part relates to the header image, and the “classes” is for setting a widescreen. Also, you shouldn’t have a space between { and %, or % and }.&lt;/p&gt;

&lt;h4 id=&quot;version-control&quot;&gt;Version Control&lt;/h4&gt;

&lt;p&gt;Just a reminder - if you followed my advice and created your source directory as a git repository, don’t forget to use git!
Do small deltas of commit + push (e.g., push it to the remote source repository once you’re done editing something on some page).&lt;/p&gt;

&lt;p&gt;You don’t want to work on something, delete it by mistake, and then find out that there’s no way to restore it :wink:.&lt;/p&gt;

&lt;h2 id=&quot;publish-your-homepage&quot;&gt;Publish Your Homepage&lt;/h2&gt;

&lt;p&gt;Once you’re satisfied with your website, i.e., you added content and even checked all the links and such, you probably want to publish it, right? :smile:.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Note that you can view it as if you’re using another device type. See instructions for Chrome &lt;a href=&quot;https://www.browserstack.com/guide/view-mobile-version-of-website-on-chrome&quot;&gt;here&lt;/a&gt;, for Safari &lt;a href=&quot;https://www.browserstack.com/guide/use-safari-devtools-to-view-mobile-web-pages&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You should build your site with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;JEKYLL_ENV=production bundle exec jekyll build
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;as instructed &lt;a href=&quot;https://jekyllrb.com/docs/step-by-step/01-setup/#build&quot;&gt;here&lt;/a&gt;. Note that if you’re running “bundle exec jekyll serve”, the site created isn’t the same as the one created with “build”.
Also, we need JEKYLL_ENV=production - more details &lt;a href=&quot;https://jekyllrb.com/docs/configuration/environments/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then, copy the generated site to your local /Repos/&amp;lt; your GitHub username &amp;gt;.github.io directory, and push it to GitHub. Good luck :crossed_fingers:.&lt;/p&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html">In this post, I’ll share how I built this website, i.e., one hosted in GitHub Pages and powered by Jekyll &amp;amp; Minimal Mistakes.</summary></entry></feed>