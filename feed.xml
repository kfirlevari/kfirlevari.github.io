<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.2">Jekyll</generator><link href="https://kfirlevari.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kfirlevari.github.io/" rel="alternate" type="text/html" /><updated>2022-10-01T19:53:30-07:00</updated><id>https://kfirlevari.github.io/feed.xml</id><title type="html">Kfir Lev-Ari</title><subtitle>Kfir Lev-Ari&apos;s Homepage</subtitle><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><entry><title type="html">Distributed DNN Training - Performance Modeling</title><link href="https://kfirlevari.github.io/DNN-Performance-Model/" rel="alternate" type="text/html" title="Distributed DNN Training - Performance Modeling" /><published>2022-01-22T00:00:00-08:00</published><updated>2022-01-22T00:00:00-08:00</updated><id>https://kfirlevari.github.io/DNN-Performance-Model</id><content type="html" xml:base="https://kfirlevari.github.io/DNN-Performance-Model/">&lt;script src=&quot;https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML&quot; type=&quot;text/javascript&quot;&gt;&lt;/script&gt;

&lt;p&gt;After looking into distributed training, I wondered what the main approaches to evaluate distributed algorithms’ performance are. Here I’ll write a summary of what I found.&lt;/p&gt;

&lt;h2 id=&quot;intro&quot;&gt;Intro&lt;/h2&gt;

&lt;p&gt;This post covers state-of-the-art performance models for distributed training algorithms. See &lt;a href=&quot;/DNN-Training/&quot;&gt;Distributed DNN Training for Practitioners&lt;/a&gt; for a summary on distributed DNN training.&lt;/p&gt;

&lt;p&gt;We focus here on performance models for predicting the execution time of a single epoch, i.e., a single forwards and backward pass through all of the training data. Predicting the required number of epochs to reach a desired level of accuracy is a different question that the following models don’t address.&lt;/p&gt;

&lt;p&gt;The structure of this post is as follows -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Sections &lt;a href=&quot;#paleo-model&quot;&gt;Paleo Model&lt;/a&gt;, &lt;a href=&quot;#feng-yan-et-al-model&quot;&gt;Feng Yan et al. Model&lt;/a&gt;, and &lt;a href=&quot;#shaohuai-shi-et-al-model&quot;&gt;Shaohuai Shi et al. Model&lt;/a&gt; give a short overview of analytical performance models for DNN algorithms.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#daniel-justus-et-al-model&quot;&gt;Daniel Justus et al. Model&lt;/a&gt; describes a performance prediction tool for DNN algorithms.&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;paleo-model&quot;&gt;Paleo Model&lt;/h2&gt;

&lt;p&gt;Paleo &lt;a class=&quot;citation&quot; href=&quot;#Qi2017PaleoAP&quot;&gt;[1]&lt;/a&gt; models the expected scalability and performance of a DNN system by extracting resource requirements from a given architecture and mapping them to a point in a design space of software, hardware and communication strategies. &lt;a href=&quot;#Figure1&quot;&gt;Figure 1.&lt;/a&gt; illustrates the different levels of execution time construction.&lt;/p&gt;

&lt;p&gt;Generally, Paleo estimation for the epoch time is a linear calculation primarily based on the number of floating-point operations performed within an epoch.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure1&quot;&gt;
  &lt;img src=&quot;/assets/images/Performance-Modeling-Images/Figure1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 1. The execution time is decomposed to computation and communication time; both are estimated for each pass of a neural network’s evaluation given user-specified choices within the design space of algorithms, hardware, and communications strategies.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#Qi2017PaleoAP&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;We now give an overview of the calculation components:&lt;/p&gt;

&lt;h3 id=&quot;computation-modeling&quot;&gt;Computation Modeling&lt;/h3&gt;

&lt;p&gt;For a single machine - DNN computation is expressed as a direct graph in which nodes are associated with operations on devices, and edges represent operation execution dependencies (dependencies in the sense that an operation can be executed only after the execution of all operations pointing to it is over).&lt;/p&gt;

&lt;p&gt;Each layer in the DNN is modeled as a node, and the connections between layers are edges. Let us denote the immediate parents of a node (i.e., layer) \(u\) with \(Pa(u)\).&lt;/p&gt;

&lt;p&gt;The overall computation time \(T(N)\) for a DNN network is built as follows:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The computation time \(T(u)\) for a single layer u that is associated with an operation \(f\) on device \(d\) consists of: 
    1. Time to fetch the input that was reproduced by the layer’s paraders, denoted \(R(Pa(u))\).
    2. Time to perform the computation of the operation, denoted \(C(f,d)\).
    3. Time to write to the local memory the outputs of the operation, denoted \(W(f,d)\).
  If we consider a sequential execution, we get: \(T(u)=R(Pa(u))+C(f,d)+W(f,d)\).
    &lt;ul&gt;
      &lt;li&gt;\(C(f,d)\)  is calculated by \(f\)’s floating-point operation counts divided by \(d\)’s floating-points operations per second (i.e., computation time = FLOP / (FLOP/sec))&lt;/li&gt;
      &lt;li&gt;\(R(Pa(u))\) and \(W(f,d)\) are calculated by the required memory size divided by \(d\)’s IO bandwidth or by the communication bandwidth in case of fetching inputs from other devices.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The computation time \(T(N)\) for a DNN network \(N\) depends on the structure of \(N\).
    &lt;ul&gt;
      &lt;li&gt;For the sequential case, \(T(N)=\sum{T(u)}\) .&lt;/li&gt;
      &lt;li&gt;For the parallel case (i.e., two or more layers that can be processed concurrently), \(N\) is reduced to a sequential network at which subgraphs of parallel layers are represented by a single super-node \(U\) (i.e., the synchronization barriers in \(N\) are the boundaries of the squashed subgraphs). The execution time of \(U\) is within the range of the best-case (completely parallel execution of the corresponding sub-graph) and worst-case (sequential execution of that sub-graph), i.e., \(T(U) \in [parallel\ execution, \ sequential\ execution]\). The parallel execution time is equal to the \(max(T(u))\) for layer \(u\) in the subgraph represented by \(U\).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note that for most types of layer operations, it’s relatively easy to reason about its FLOP counts. For convolutional layers, the exact counts number is derived from the underlying algorithm used (two main algorithms are matrix multiplication and FFT, each with different complexity and IO overhead. For more info, see Section 3.1.3 in &lt;a class=&quot;citation&quot; href=&quot;#Qi2017PaleoAP&quot;&gt;[1]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Generally, Paleo uses heuristics that are based on offline benchmarks to estimate what underlying algorithm was selected since the selection depends on filter size, strides, input size of convolutional layers, and available memory.&lt;/p&gt;

&lt;h3 id=&quot;communication-modeling&quot;&gt;Communication Modeling&lt;/h3&gt;

&lt;p&gt;For communication between two workers, the communication time \(T_{comm}\)​ is equal to the size of the data \(∣D∣\) to be communicated between them, divided by the bandwidth of the communication channel \(B\) , i.e., \(T_{comm} = \frac{|D∣}{B}\).&lt;br /&gt;
The overall communication time \(T\) among \(K\) workers depends on the communication scheme used (Paleo consider these three schemes):&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;OneToAll - \(2KT_{comm}\)​&lt;/li&gt;
  &lt;li&gt;Tree AllReduce -  \(2log2​(K)T_{comm}\)​&lt;/li&gt;
  &lt;li&gt;Butterfly AllReduce - \(log2​(K)T_{comm}\)​&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;platform-percent-of-peak&quot;&gt;Platform Percent of Peak&lt;/h3&gt;

&lt;p&gt;In a perfect scenario, all devices operate at their peak FLOPS, and network and IO links are fully saturated. Since it’s unreasonable in practice, Paleo suggests a scaling constant called &lt;em&gt;platform percent of peak (PPP)&lt;/em&gt; to capture the inefficiency of a platform compared to peak FLOPS (100% is the perfect scenario). PPP is calculated by running a small benchmark (on a single GPU for computation PPP and for communication PPP).&lt;/p&gt;

&lt;h3 id=&quot;discussion&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Model results seem very close to the actual execution time&lt;/li&gt;
  &lt;li&gt;Friendly open-source graphical interface  &lt;a class=&quot;citation&quot; href=&quot;#Qi2017PaleoAPGitHub&quot;&gt;[2]&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The paper is from 2017, so it doesn’t consider newer algorithms, e.g.:
    &lt;ul&gt;
      &lt;li&gt;Consider only data and model parallelism - pipelining isn’t in the paper or hybrid approaches.&lt;/li&gt;
      &lt;li&gt;Do not consider the recently introduced communication schemes, such as a butterfly mixing scheme or non-deterministic asynchronous parameter servers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;A linear model does not necessarily capture the complexity of the problem (see &lt;a class=&quot;citation&quot; href=&quot;#Justus2018PredictingTC&quot;&gt;[3]&lt;/a&gt; for more information about the flaws of a linear FLOPs-based estimation).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;feng-yan-et-al-model&quot;&gt;Feng Yan et al. Model&lt;/h2&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#10.1145/2783258.2783270&quot;&gt;[4]&lt;/a&gt;, they suggest a performance model for predicting the efficiency of a given configuration, i.e., predicting the training epoch duration when using that configuration. Later, they use this model for reducing the search time for efficient and accurate configuration - by empirically evaluating efficient configurations (the idea is that searching a good configuration this way is faster since we iterate over efficient configurations).&lt;/p&gt;

&lt;p&gt;Next, we give intuition to their performance model (see Section 4. in &lt;a class=&quot;citation&quot; href=&quot;#10.1145/2783258.2783270&quot;&gt;[4]&lt;/a&gt; for the full description and equations). Generally, the performance prediction calculation is based on multiple small benchmark results.&lt;/p&gt;

&lt;p&gt;This model integrates Model Parallelism and Data Parallelism - we’ll start by showing a model for the training time of a single sample across multiple machines (as done in Model Parallelism), and then we’ll extend this model to consider different samples on different machines (i.e., to capture Data Parallelism).&lt;/p&gt;

&lt;h3 id=&quot;model--parallelism&quot;&gt;Model  Parallelism&lt;/h3&gt;

&lt;p&gt;Each layer is partitioned into segments that are processed in parallel, meaning that the layer’s execution time is determined by the slowest segment. If we assume that the layer is partitioned into equal segments and also that each segment is processed by the same processing power, then we can estimate the layer’s training time by using any of its segments. The total training time of a segment is equal to the sum of its feed-forward evaluation, back-propagation, and weight updates (each of these consists of computation and communication):&lt;/p&gt;

&lt;h4 id=&quot;feed-forward-evaluation&quot;&gt;Feed-forward Evaluation&lt;/h4&gt;

&lt;p&gt;For each segment, the feed-forward evaluation time is equal to the time spent in computing the output activations of the neurons in the segment and communicating activations from the connected segments in the previous layer.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computation: they suggest evaluating this duration for a given machine by running a micro-benchmark that consists of a canonical feed-forward evaluation. Generally, this benchmark iterates over the neurons of a segment, and for each neuron: (1) accumulates activations (each action is a multiply-add, based on the previous layer activations and the corresponding weight), and (2) computes an activation function for the neuron using the sum that was calculated in (1). See &lt;a href=&quot;#alg1&quot;&gt;Algorithm 1.&lt;/a&gt; for the pseudo-code.&lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;&quot; id=&quot;alg1&quot;&gt;
  &lt;img src=&quot;/assets/images/Performance-Modeling-Images/Alg1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Algorithm 1. Canonical feed-forward evaluation.
Algorithm adapted from &lt;a class=&quot;citation&quot; href=&quot;#10.1145/2783258.2783270&quot;&gt;[4]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;Communication: given that activations can be sent asynchronously, they assume that the communication time of feed-forward evaluation is dominated by the delay in receiving cross-machine activations from the previous layer. 
  Thus, the communication time is equal to the sum of (1) network latency of sending one bit of data between two workers and (2) the transfer time of the data size received by segment (i.e., the size of each activation multiplied by the number of remote activations and divided by the bandwidth of the machine’s NIC).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;back-propagation&quot;&gt;Back-propagation&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Computation: since the calculation structure is similar to the feed-forward calculation, the cost estimation is also similar.&lt;/li&gt;
  &lt;li&gt;Communication: here, we follow similar reasoning to the one we showed in the communication of forward-feed evaluation, only this time, the data that is received by the segment consists of errors from the following layer, instead of activations from the previous layer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;weight-updates&quot;&gt;Weight Updates&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Computation: when updating the segment’s weights, for each weight between two connected neurons (i.e., one that belongs to the segment and another that belongs to another segment), a weight delta is computed using an error term and an activation value (a multiplication action). Then this delta is added to the weight. This means that the total computation cost of weight updates is these multiplications and add actions, multiplied by the number of connections between the segment’s neurons to other segments’ neurons.&lt;/li&gt;
  &lt;li&gt;Communication: at this point, when we only consider model parallelism, weights aren’t communicated; therefore, the communication cost is 0.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data-parallelism&quot;&gt;Data Parallelism&lt;/h3&gt;

&lt;p&gt;We now show the required modifications to the model above for supporting concurrent processing of multiple samples, as done in data parallelism. We focus on three forms - (1) chip-level multiprocessing, (2) layer replication, and (3) model replicas with parameter servers.&lt;/p&gt;

&lt;h4 id=&quot;chip-level-multiprocessing&quot;&gt;Chip-level Multiprocessing&lt;/h4&gt;

&lt;p&gt;Different cores of a chip-level multiprocessing system can process different samples concurrently (e.g., a 16 core processing 16 samples at a time) while asynchronously sharing weights through shared memory. The new dimension here is the number of concurrent threads: a higher value increases concurrency but also increases the potential interference among the threads. Now each segment will be identified by its layer, partition number, and thread ID.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computation: Concurrent training of multiple samples may interfere with each other, competing for memory bandwidth and thus affecting per-sample computation time. For a given number of threads, a performance interference factor is defined to model the interference among them. We estimate it by running a multi-threaded version of the canonical form such that each thread processes the same code segment using different cores. The interference factor then is estimated as the ratio of the multi-thread execution time and the single-thread execution time. The new estimation for computation time of the segment is received by multiplying this interference factor by the estimated computation time of the segment we defined in the model parallelism model (i.e., in a single thread execution).
    &lt;ul&gt;
      &lt;li&gt;Data parallelism degree - is defined as the training concurrency degree for a given layer. When we only consider threads concurrency, the degree is equal to the number of threads (this component will later be used for calculating the overall training time).&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Communication: when training multiple samples with multiple threads, the network bandwidth is shared among threads, i.e., each thread gets an equal portion of the bandwidth. This affects the data transfer calculation as we now have less bandwidth (a function of the number of threads). In addition, the latency is also a function of the threads number since it might increase due to resource allocation on each side (for supporting multiple connections).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;layer-replication&quot;&gt;Layer Replication&lt;/h4&gt;

&lt;p&gt;In layer replication, multiple machines process a different subset of training data on the same part of the network, thus reducing the network overheads of cross-machine activations. The replication factor doesn’t affect the computation time of each segment or the communication time.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Data parallelism degree - when adding replication, the data parallelism degree is equal to the number of threads multiplied by the replication number.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;multiple-model-replicas&quot;&gt;Multiple Model Replicas&lt;/h4&gt;

&lt;p&gt;We now add to the performance model the scenario in which model replicas are trained in parallel using a sharded parameter server scheme. The weight communication load scales with the number of model replicas (since each replica induces more communication load on the parameter servers), and the optimal ratio between computation and communication depends on both the DNN and hardware.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Computation: same as before, since a single sample computation isn’t affected by model replication.
    &lt;ul&gt;
      &lt;li&gt;Data parallelism degree - extended by a multiplication factor equals the number of model replicas.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Communication: while writing weights to parameter servers can be done asynchronously and does not affect training time, reading weights is a blocking operation since a replica must wait for the weights in order to train the model.  Therefore we add the reading time to the model. The worst-case for the reading time is when all replicas read from the same parameter server at the same time. In this case, the data size (that is read from the parameter server) is equal to the number of replicas multiplied by the size of the model weights, and the transfer time is that data size divided by the bandwidth of a single thread (assuming multiple threads on the parameter server). In addition, we consider the latency between the replica and the parameter server to be also a function of the thread numbers. The actual time is lower than the worst-case (in the paper, they also state the best case as the another bound).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;epoch-time&quot;&gt;Epoch Time&lt;/h3&gt;

&lt;p&gt;In conclusion, the total estimated epoch time in this performance model is equal to the sum of the following components:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;The time it takes to read the model from the parameter server multiplied by the total number of times a replica reads from the parameter servers (the latter number is equal to the total number of samples divided by the number of samples used by a replica before reading from the parameter server).&lt;/li&gt;
  &lt;li&gt;The total time it takes to train all layers. For a given layer, the time is equal to the total number of samples divided by the data parallelism degree (i.e., this gives us a factor for sequential segments - higher number means that we have more sequential segments), multiplied by the time it takes to process a single segment. Processing a segment is the sum of (1) feed-forward propagation computation and communication, (2) back-propagation computation and communication, and (3) update weights computation.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;discussion-1&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Their optimizer, which is using their model, was able to find an optimal configuration (for benchmarks) efficiently.&lt;/li&gt;
  &lt;li&gt;The model takes into account also custom hardware (e.g., FPGAs, ASICs, and RDMA)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It seems like multiple benchmark executions are required in order to get the right estimators, e.g., at least one benchmark execution for every tested number of threads.&lt;/li&gt;
  &lt;li&gt;It doesn’t consider the decentralized approach of model parallelism.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;shaohuai-shi-et-al-model&quot;&gt;Shaohuai Shi et al. Model&lt;/h2&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#Shi2018PerformanceMA&quot;&gt;[5]&lt;/a&gt;, the authors analyze frameworks’ training time while running on multiple GPUs and nodes. To that end, they build a high-level performance model for training DNNs with synchronous SGD (S-SGD) on GPUs. Generally, their model divides the iteration time into its main components and later measures them as they evaluate the performance.&lt;/p&gt;

&lt;h3 id=&quot;workflow-of-sgd-and-s-sgd&quot;&gt;Workflow of SGD and S-SGD&lt;/h3&gt;

&lt;p&gt;When using a single GPU, the main components of a mini-batch SGD iteration duration are:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Read duration of data from disk to CPU memory&lt;/li&gt;
  &lt;li&gt;Data transfer duration from CPU to GPU&lt;/li&gt;
  &lt;li&gt;GPU forward feed duration&lt;/li&gt;
  &lt;li&gt;GPU backward propagation duration&lt;/li&gt;
  &lt;li&gt;Duration of updating the model using the gradients&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;When extended to S-SGD (i.e., for using multiple GPUs and nodes), we have the following phases:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Each node reads the same amount of samples.&lt;/li&gt;
  &lt;li&gt;Each node distributed the samples evenly to all of its GPUs.&lt;/li&gt;
  &lt;li&gt;Each GPU runs forward feeds and back propagations.&lt;/li&gt;
  &lt;li&gt;The gradients are averaged among all node’s GPUs - there is synchronization among GPUs as they must wait for all to complete this step.&lt;/li&gt;
  &lt;li&gt;Each GPU updates its parameters.&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;performance-model&quot;&gt;Performance Model&lt;/h3&gt;

&lt;p&gt;The iteration time titer​ can be written as a sum of three components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;\(t_{io}\)​ - I/O duration in that iteration - reading data from disk to CPU memory.&lt;/li&gt;
  &lt;li&gt;\(t_{gpu}\)​ - a duration that sums the work of the GPU. It includes the data transfer time from CPU to GPU, the forward feeds, the back propagations, and the model update operations.&lt;/li&gt;
  &lt;li&gt;\(t_{comm}\)​ - duration of gradients communication (equals 0 if we only use one GPU).&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using pipelining, we can partially hide tio​ and \(t_{comm}\)​, as we now explain.&lt;/p&gt;

&lt;h4 id=&quot;hiding-io-duration&quot;&gt;Hiding I/O Duration&lt;/h4&gt;

&lt;p&gt;Reducing the I/O duration in step 1 is done by reading new data into CPU memory in parallel with a previous iteration’s computation and communication steps.
By doing so, the average iteration time becomes \(t_{iter}​=max(t_{gpu}​+t_{comm}​,t_{io}​)\)&lt;/p&gt;

&lt;h4 id=&quot;hiding-communication-duration&quot;&gt;Hiding Communication Duration&lt;/h4&gt;

&lt;p&gt;In each layer, the gradient computation has no dependency on updating the following layers, which means that this computation can be parallelized with the communication (i.e., gradient aggregation in phase 4) in the next layer. The exact level of parallelism depends on the duration of the computation and the duration of the communication.&lt;/p&gt;

&lt;h3 id=&quot;discussion-2&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A simple model that can express the expected speedup for a given number of GPUs.&lt;/li&gt;
  &lt;li&gt;The effect of I/O and communication on scalability is visible when examining the model equations.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Mostly a guideline for different time components that need to be measured, most of the work is measuring the different durations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;daniel-justus-et-al-model&quot;&gt;Daniel Justus et al. Model&lt;/h2&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#Justus2018PredictingTC&quot;&gt;[3]&lt;/a&gt;, the authors propose to use a deep neural network to predict the training execution time of a single layer and then build the total expected execution time by summing the results of all layers. They suggest training that predictor DNN using features that are derived from the computational resource used, the network that is trained, and the data used for training. They categorized these features into layer features, layer-specific features, and hardware features. Due to the size of the resulting feature space, they trained their predictor DNN via random subsamples. We now give a summary of the features they mentioned in the paper:&lt;/p&gt;

&lt;h3 id=&quot;layer-features&quot;&gt;Layer Features&lt;/h3&gt;

&lt;p&gt;This category mainly covers the layer’s hyper-parameters, such as:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Activation function type, e.g., ReLU, softmax, sigmoid, etc…&lt;/li&gt;
  &lt;li&gt;Optimizer type, e.g., Gradient Descent, Adadelta, Adagrad, Momentum, Adam, and RMS Prop.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;layer-specific-features&quot;&gt;Layer Specific Features&lt;/h3&gt;

&lt;p&gt;This category covers features that correspond to specific layer types. Here is a list of a layer type and the features that can be used for it:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Fully connected layer - number of inputs to that layer, number of neurons in that layer.&lt;/li&gt;
  &lt;li&gt;Convolutional layer - matrix size, kernel size, input number of channels, output number of channels, stride size, input padding size&lt;/li&gt;
  &lt;li&gt;Pooling layer - kernel size, stride size, padding size&lt;/li&gt;
  &lt;li&gt;Recurrent features - same as a fully connected layer, and also type of recurrence (e.g., LSTM, GRU) and a binary value indicating if the RNN is bidirectional&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;hardware-features&quot;&gt;Hardware Features&lt;/h3&gt;

&lt;p&gt;The features in this category describe hardware characteristics. For example, for GPU, we can have the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Type, e.g., NVIDIA w/ Turing microarchitecture, or Volta, etc…&lt;/li&gt;
  &lt;li&gt;Count (number of GPUs used)&lt;/li&gt;
  &lt;li&gt;Memory size&lt;/li&gt;
  &lt;li&gt;Clock speed&lt;/li&gt;
  &lt;li&gt;Memory bandwidth&lt;/li&gt;
  &lt;li&gt;Core count&lt;/li&gt;
  &lt;li&gt;GFLOPS peak performance&lt;/li&gt;
  &lt;li&gt;Connectivity type&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;See Section 5. in &lt;a class=&quot;citation&quot; href=&quot;#Justus2018PredictingTC&quot;&gt;[3]&lt;/a&gt; for details about the methodology they used while training their predictor model.&lt;/p&gt;

&lt;h3 id=&quot;discussion-3&quot;&gt;Discussion&lt;/h3&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Open source code &lt;a class=&quot;citation&quot; href=&quot;#Justus2018PredictingTCGitHub&quot;&gt;[6]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Their trained model accurately predicted the required execution time for a wide range of the most frequently used components of neural networks.&lt;/li&gt;
  &lt;li&gt;It can be extended as much as needed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It probably requires a long preparation phase before the approach is usable, compared to the previous approaches.&lt;/li&gt;
  &lt;li&gt;Predictions are less explainable compared to other models.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusions&quot;&gt;Conclusions&lt;/h2&gt;

&lt;p&gt;In this post, we saw four performance models, from a model that requires the least amount of benchmarking (Paleo - that bases the training time expectation on the cost of FLOP operations) to a model that requires the most (the last model that requires a dataset of whole training sessions and corresponding durations).&lt;/p&gt;

&lt;h2 id=&quot;refrences&quot;&gt;Refrences&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;Qi2017PaleoAP&quot;&gt;Hang Qi, Evan R. Sparks, and Ameet Talwalkar. &lt;b&gt;Paleo: A Performance Model for Deep Neural Networks&lt;/b&gt;. In: &lt;i&gt;5th International Conference on Learning Representations, ICLR 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings&lt;/i&gt;, OpenReview.net, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Qi2017PaleoAPGitHub&quot;&gt;Hang Qi, Evan R. Sparks, and Ameet S. Talwalkar. &lt;b&gt;Paleo - An analytical model to estimate the scalability and performance of deep learning systems &lt;a href=&quot;https://talwalkarlab.github.io/paleo/&quot;&gt;link&lt;/a&gt;&lt;/b&gt;, 2107.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Justus2018PredictingTC&quot;&gt;Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. &lt;b&gt;Predicting the Computational Cost of Deep Learning Models&lt;/b&gt;. &lt;i&gt;2018 IEEE International Conference on Big Data (Big Data)&lt;/i&gt;, :3873–3882, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;10.1145/2783258.2783270&quot;&gt;Feng Yan, Olatunji Ruwase, Yuxiong He, and Trishul Chilimbi. &lt;b&gt;Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems&lt;/b&gt;. In: &lt;i&gt;Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining&lt;/i&gt;, pp. 1355–1364, Association for Computing Machinery, New York, NY, USA, 2015.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Shi2018PerformanceMA&quot;&gt;Shaohuai Shi and Xiaowen Chu. &lt;b&gt;Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs&lt;/b&gt;. &lt;i&gt;2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)&lt;/i&gt;, :949–957, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Justus2018PredictingTCGitHub&quot;&gt;Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. &lt;b&gt;performance-prediction &lt;a href=&quot;https://github.com/CDECatapult/ml-performance-prediction&quot;&gt;link&lt;/a&gt;&lt;/b&gt;, 2018.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html"></summary></entry><entry><title type="html">Machine Learning Knowledge Graph</title><link href="https://kfirlevari.github.io/ML-World/" rel="alternate" type="text/html" title="Machine Learning Knowledge Graph" /><published>2022-01-15T00:00:00-08:00</published><updated>2022-01-15T00:00:00-08:00</updated><id>https://kfirlevari.github.io/ML-World</id><content type="html" xml:base="https://kfirlevari.github.io/ML-World/">&lt;link rel=&quot;stylesheet&quot; href=&quot;/assets/css/ml-world-post/ml-dag.css&quot; /&gt;

&lt;p&gt;Anat (my wife) told me that it would be nice to have a graphical representation of the different terms and domains in ML. I remember seeing static images of that type online, but I didn’t find a dynamic knowledge graph that gives a clearer picture for a selected term.
I thought it would be interesting to implement one as a weekend project, and this post is the result of that attempt :crossed_fingers:.&lt;/p&gt;

&lt;p&gt;Generally, the first step was to create (manually) &lt;a href=&quot;/assets/datasets/ml-world.json&quot;&gt;this&lt;/a&gt; small file, based on Wikipedia. It consists of the main ML terms and their relations. I intend to extend it soon since the first version covers just the basics. In addition, I will happily expand/fix it based on any suggestion I’ll get in the comments :wink:.&lt;/p&gt;

&lt;p&gt;Then, I used javascript magic (mostly dark magic, &lt;a href=&quot;/assets/js/ml-world-post/ml-dag.js&quot;&gt;here&lt;/a&gt;) to generate the graph, fuzzy search, and optional Wikipedia previews.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;The first impression you might get is -&lt;/p&gt;
&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/ml-world/cant-see.jpeg&quot; alt=&quot;&quot; style=&quot;width: 25%; margin: auto&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;That’s because without selecting any term, you simply see the whole picture (no pun intended).&lt;/p&gt;

&lt;p&gt;Note that the javascript code doesn’t support touch screens at this point, so I would recommend viewing this graph on a device that has a touchpad/mouse.&lt;/p&gt;

&lt;p&gt;You have the following tools here that should help make the graph more usable:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Use the mouse-wheel to zoom in and out.&lt;/li&gt;
  &lt;li&gt;Click on any node to focus on the specific term - you’ll see its parents and direct children.&lt;/li&gt;
  &lt;li&gt;To move the whole graph - click on any location that is not a node and drag it.&lt;/li&gt;
  &lt;li&gt;Select the WikiPreview option to see a summary from Wikipedia when hovering above a node.&lt;/li&gt;
  &lt;li&gt;And last but not least, use the search box.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;input type=&quot;checkbox&quot; id=&quot;myCheckbox&quot; /&gt; Show WikiPreview&lt;/p&gt;

&lt;div class=&quot;autocomplete&quot;&gt;
    &lt;input id=&quot;search&quot; type=&quot;text&quot; placeholder=&quot;Search..&quot; /&gt;
&lt;/div&gt;

&lt;p class=&quot;content&quot;&gt;
&lt;svg&gt;&lt;/svg&gt;
&lt;/p&gt;
&lt;div class=&quot;popup-container&quot;&gt;&lt;/div&gt;

&lt;script src=&quot;/assets/js/wikipedia-preview.production.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://cdn.jsdelivr.net/npm/fuse.js@6.5.3&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://d3js.org/d3.v7.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://unpkg.com/d3-dag@0.9.0&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;/assets/js/ml-world-post/ml-dag.js&quot;&gt;&lt;/script&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html"></summary></entry><entry><title type="html">Distributed Deep Neural Network (DNN) Training for Practitioners</title><link href="https://kfirlevari.github.io/DNN-Training/" rel="alternate" type="text/html" title="Distributed Deep Neural Network (DNN) Training for Practitioners" /><published>2022-01-11T00:00:00-08:00</published><updated>2022-01-11T00:00:00-08:00</updated><id>https://kfirlevari.github.io/DNN-Training</id><content type="html" xml:base="https://kfirlevari.github.io/DNN-Training/">&lt;p&gt;I love both distributed systems and machine learning. However, when I started grad school, I wasn’t sure which one to choose. In my first semester, the ML 101 course wasn’t taught while  parallel computing 101 did, and so I decided based on availability :smile:&lt;/p&gt;

&lt;p&gt;This topic of distributed training fascinates me because it combines essential aspects of both domains.&lt;/p&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;In this survey, we cover strategies and techniques related to distributed DNN training.
The goal is to explore/highlight aspects of state-of-the-art approaches and make them more accessible to practitioners.&lt;/p&gt;

&lt;p&gt;Some basic knowledge in deep neural networks is required (if you’re missing it, you can see Section 2 and Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and/or Section 2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The structure of this survey is as follows -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Section &lt;a href=&quot;#dimensions-of-concurrency&quot;&gt;Dimensions of Concurrency&lt;/a&gt; gives an overview of the different dimensions of the parallel &amp;amp; distributed training problem.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#network-level-optimizations&quot;&gt;Network Level Optimizations&lt;/a&gt; focuses on the network aspect of training and related optimizations.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#hardware-level-optimizations&quot;&gt;Hardware Level Optimizations&lt;/a&gt; discusses CPU, GPU, and switch optimizations.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#fault-tolerance&quot;&gt;Fault Tolerance&lt;/a&gt; touches on a few techniques used by distributed training systems to deal with failures.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#from-ideas-to-practice&quot;&gt;From Ideas to Practice&lt;/a&gt; maps the ideas presented in this survey to available tools.&lt;/li&gt;
  &lt;li&gt;Section &lt;a href=&quot;#building-a-training-service---related-topics&quot;&gt;Building a Training Service - Related Topics&lt;/a&gt; notes more directions to consider when building a training service&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;dimensions-of-concurrency&quot;&gt;Dimensions of Concurrency&lt;/h2&gt;

&lt;p&gt;In this section, we give a short overview of the three primary computation levels found in DNN training that can be improved with concurrency - we start with the basic unit of a single DNN operator (Section &lt;a href=&quot;#single-dnn-operator-computation&quot;&gt;Single DNN Operator Computation&lt;/a&gt;), then look at a bigger picture that consists of the DNN structure itself (Section &lt;a href=&quot;#dnn-model-computation&quot;&gt;DNN Model Computation&lt;/a&gt;), and finish with the highest level which is the overall training flow (Section &lt;a href=&quot;#handling-multiple-copies-of-a-model&quot;&gt;Handling Multiple Copies of a Model&lt;/a&gt;).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure1&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 1. Dimensions of Concurrency in DNN Training. On the left - single operator computation, in the middle - single model evaluation (via forward evaluation &amp;amp; back propagation), and on the right - maintaining multiple states of the same model.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;single-dnn-operator-computation&quot;&gt;Single DNN Operator Computation&lt;/h3&gt;

&lt;p&gt;The main DNN operator types are - 1. Activation, 2. Fully Connected, 3. Convolution, 4. Pooling, and 5. Batch Normalization. 
Generally, the opportunities for parallelization in their computation are rooted in the locality of the operations that are done as part of each operator and the dimensionality of the data. The data dimensions are - 1. the minibatch size, 2. the number of channels/features/neurons, 3. the single sample dimensions (i.e., height, width), and 4. the convolution kernel dimensions (in convolution operator).&lt;/p&gt;

&lt;p&gt;At their core, nearly all operators are computed via matrix multiplication, and many optimizations in the literature focus on that. For more info, see Section 5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;dnn-model-computation&quot;&gt;DNN Model Computation&lt;/h3&gt;

&lt;p&gt;DNN models are created using SGD (see &lt;a href=&quot;#alg1&quot;&gt;Alg. 1.&lt;/a&gt;). This calculation can be partitioned among multiple processors due to the following dimensions: 1. the minibatch granularity, 2. the breath of the layers, and 3. the depth of the DNN.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;alg1&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Alg1.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Alg. 1. Minibatch Stochastic Gradient Descent (SGD) with Backpropagation.
Algorithm adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;There are three prominent schemes for partitioning as depicted in &lt;a href=&quot;#Figure2&quot;&gt;Figure 2.&lt;/a&gt;: 1. partition by input samples (data parallelism), 2. by network structure (model parallelism), and 3. by layer (pipelining). In addition, there’s a hybrid approach that combines multiple schemes to overcome the drawbacks of each method.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure2&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure2.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 2. DNN Parallelism Schemes.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h4 id=&quot;data-parallelism&quot;&gt;Data Parallelism&lt;/h4&gt;

&lt;p&gt;In this approach, minibatches are partitioned among multiple computational resources. 
Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Simple to reason about.&lt;/li&gt;
  &lt;li&gt;All operators except Batch Normalization operate on a single sample. Therefore in this approach, we can run forward evaluation and backpropagation almost independently.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;This approach requires each resource to maintain the whole model, as each sample might modify any of the model’s parameters.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info, see Section 6.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;model-parallelism&quot;&gt;Model Parallelism&lt;/h4&gt;

&lt;p&gt;In this approach, different parts of the DNN are computed on different computational resources. 
Pros:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Requires less memory from each computation resource (no need to store the whole model locally)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The minibatches are copied to all computation resources.&lt;/li&gt;
  &lt;li&gt;Communication intensive (e.g., handling a fully connected layer) significantly affects performance.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Generally, effectively splitting models isn’t easy, and stalling due to communication overhead or synchronizations may occur. This means that increasing the degree of model parallelism doesn’t necessarily lead to training speedup (as in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/icml/MirhoseiniPLSLZ17&quot;&gt;[3]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;For more info, see Section 6.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;layer-pipelining&quot;&gt;Layer Pipelining&lt;/h4&gt;

&lt;p&gt;Pipelining can refer to :&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Overlapping computations of layers - overlapping the steps of forward evaluation, backpropagation, and weight updates.
    &lt;ul&gt;
      &lt;li&gt;Pros: mitigates processor idle time.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Layer partitioning - partitioning the DNN according to its depth to different computational resources
    &lt;ul&gt;
      &lt;li&gt;Pros: 1. no need to store the whole model locally, 2. fixed number of communication points between processors.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons of pipelining -&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Sample arrival rate affects system utilization, and 2. latency proportional to the number of processors.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For more info, see Section 6.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 3.2.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;handling-multiple-copies-of-a-model&quot;&gt;Handling Multiple Copies of a Model&lt;/h3&gt;

&lt;p&gt;In a distributed environment, there are multiple instances of SGD running independently, and therefore there are multiple copies of the model. The different distributed training algorithms can be categories by the way they address the following domains: 1. &lt;a href=&quot;#model-consistency&quot;&gt;Model Consistency&lt;/a&gt;, 2. &lt;a href=&quot;#centralization&quot;&gt;Centralization&lt;/a&gt;, 3.&lt;a href=&quot;#parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/a&gt;, 4. &lt;a href=&quot;#optimization-algorithms&quot;&gt;Optimization Algorithms&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;model-consistency&quot;&gt;Model Consistency&lt;/h4&gt;

&lt;p&gt;Consistency is essentially a tradeoff between the potential loss in model quality or convergence speed (when training instances use a stale model) and the synchronization costs (of updating instances model).&lt;/p&gt;

&lt;p&gt;If we think about consistency as an axis, then that axis describes the possible states of the model as observed by the different SGD instances before running the SGD method. We have consistent model algorithms in which everyone observes the most updated model on one end. On the other end, we have inconsistent model algorithms, in which there is no guarantee on the staleness of the model that each participant holds. In between, some algorithms bound the staleness as a compromise between consistency and inconsistency.&lt;/p&gt;

&lt;p&gt;Consistent model algorithms - Pros: simple to reason about. Cons: costly synchronization that hurts scaling.&lt;/p&gt;

&lt;p&gt;Inconsistent model algorithms - Pros: no extra synchronization is needed. Participants train almost independently. 
Cons: it doesn’t converge in all cases.&lt;/p&gt;

&lt;p&gt;Compromise approach - Pros: 1. pay for synchronization only after a bound is reached; 2. keeping lagging participants in check, and allows to add/remove on-the-fly participants;&lt;/p&gt;

&lt;p&gt;It seems like the consistent model algorithms are used nowadays for up to 35-50 participants, after which the added cost of communication breaks the linear scalability. Then, for higher participant numbers, the non-consistent model algorithms are used.&lt;/p&gt;

&lt;p&gt;There’s another dimension of model consolidation for inconsistent model algorithms that determines how the algorithm combines the resulting models of the different SGC instances - either post-training or several times during training.
A common post-training technique is ensemble learning (i.e., keeping an ensemble of models); other techniques are knowledge distillation (keeping an ensemble of smaller models that are trained based on a bigger one) and model averaging (that can also be used during training).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure3&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure3.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 3. Model consistency levels.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;For more info, see Section 7.1 and Section 7.4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;centralization&quot;&gt;Centralization&lt;/h4&gt;

&lt;p&gt;Centralization range between the extreme of a single participant that is responsible for the collection, update, and distribution of the model (denoted by parameter server PS) to the other extreme of a completely decentralized network at which all participants share the responsibility for the model equally (in this case, the participants would rely on allreduce operations to communicate model updates among themselves).&lt;/p&gt;

&lt;p&gt;The selected design choice should consider the network characteristics (topology, bandwidth, and latency) and the required level of fault tolerance.
Note that PS consists of a single node on the extreme, but it’s possible to have an ensemble of participants that act together as a PS.&lt;/p&gt;

&lt;p&gt;The tradeoff between using either distribution scheme can be modeled by the communication cost per global update.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Centralize scheme&lt;/em&gt; -&lt;/p&gt;

&lt;p&gt;Pros:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;PS can keep track of a “global view” of training&lt;/li&gt;
  &lt;li&gt;Averaging the gradients at one location&lt;/li&gt;
  &lt;li&gt;Enabling asynchronous operation mode of the training participants&lt;/li&gt;
  &lt;li&gt;Participants can communicate less information by performing some of the computations on the PS&lt;/li&gt;
  &lt;li&gt;Increases fault tolerance by dynamic spin-up and removal of nodes during training&lt;/li&gt;
  &lt;li&gt;enable handling heterogeneity, both in training agents and in network settings (e.g., latency, x-dc)&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Cons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Requires each training participant to send and receive information from a specific subset of nodes (PS nodes) - unbalanced network usage pattern, e.g., a bottleneck on the PS with the increase of participants (in some cases, can be resolved using model parallelism and pipelining on the PS side).&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;For the PS approach to scale, there’s a need to optimize software and hardware configurations and awareness of underlying physical network topology.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;One example to such optimization (at rack scale) is  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/LuoNCPK18&quot;&gt;[4]&lt;/a&gt; - they show:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;Analysis of three bottlenecks due to (1) network bandwidth, (2) software framework inefficiencies, and (3) suboptimal deployment.&lt;/li&gt;
      &lt;li&gt;Design &amp;amp; evaluate a rack-scale PS that mitigates the above issues&lt;/li&gt;
      &lt;li&gt;Design of an HW component that their PS is using and is responsible for rack-level and cross-rack gradient reductions&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Another example, is a PS algorithm that uses Stale-Sync SGD (as described in Section &lt;a href=&quot;#model-consistency&quot;&gt;Model Consistency&lt;/a&gt;) - this paper &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/HoCCLKGGGX13&quot;&gt;[5]&lt;/a&gt; shows an example of such an implementation, evaluation, and proof of convergence.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;em&gt;Decentralize scheme&lt;/em&gt; -&lt;/p&gt;

&lt;p&gt;Pros: load balancing can be achieved using asynchronous training&lt;/p&gt;

&lt;p&gt;Cons: doesn’t scale well (e.g., see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/abs-1902-06855&quot;&gt;[6]&lt;/a&gt; for scalability analysis + suggested optimizations to mitigate the issues).&lt;/p&gt;

&lt;p&gt;Examples of optimizations for improving the scalability of this approach:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;A ring-based Allreduce that was proven to be optimal in terms of bandwidth (see proof here &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/PatarasukY09&quot;&gt;[7]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;A decentralized version that is hierarchical (see Hierarchical All-reduce in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/abs-1807-11205&quot;&gt;[8]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;An adaptive algorithm that changes the number of partitions to correspond to the number of participants so that the network bandwidth remains constant (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/apsys/WangZGZ14&quot;&gt;[9]&lt;/a&gt;).&lt;/li&gt;
  &lt;li&gt;Tunable partitioning of gradient updates that enable constant network bandwidth usage (i.e., the bandwidth remains unaffected by the number of participants). For more info, see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/WatcharapichatM16&quot;&gt;[10]&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Note that in low bandwidth or high latency networks, it might be that the decentralized approach behaves better than PS since it’s able to avoid the communication traffic jam (e.g., see two key observations on page 8 of &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/LianZZHZL17&quot;&gt;[11]&lt;/a&gt;)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For more info on centralization, see Section 7.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;&lt;/p&gt;

&lt;h4 id=&quot;parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/h4&gt;

&lt;p&gt;Compression relates to techniques that reduce message size. Generally, there are two compression levels -&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;Compressing the model parameters/gradients with efficient data representations - as done via quantization (mapping continuous information into buckets that represent sets of values)&lt;/li&gt;
  &lt;li&gt;Avoid sending unnecessary information altogether (e.g., sending only relevant parts of sparse gradients), which results in communication of sparse data structures.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Note that some techniques are harder to implement in a decentralized scheme or even unique to the PS scheme (e.g., decreasing message size by sending activations and errors instead of parameters, relying on the PS computation capabilities to fill the missing computation steps).&lt;/p&gt;

&lt;p&gt;In addition, the computational cost of compression is non-negligible, and in some cases, outweighs the communication-reduction benefits (see &lt;a class=&quot;citation&quot; href=&quot;#10754/662495&quot;&gt;[13]&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;For more info, see Section 7.3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;, and 3.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;optimization-algorithms&quot;&gt;Optimization Algorithms&lt;/h4&gt;

&lt;p&gt;Multiple optimizations aim to improve the training process but do not necessarily relate to the distributed nature of the computation (still, we highlight that distributed training algorithms might leverage these optimizations).&lt;/p&gt;

&lt;p&gt;E.g., for improving the parameter search, an algorithm might use a technique that reduces the variance of SGD (variance caused by the random sampling).&lt;/p&gt;

&lt;p&gt;Another example is an algorithm that tunes its hyper-parameters during execution or an algorithm that modifies its architecture (e.g., change the structure of layers).&lt;/p&gt;

&lt;p&gt;For more info, see Section 7.5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;network-level-optimizations&quot;&gt;Network Level Optimizations&lt;/h2&gt;

&lt;p&gt;Given the cluster size and compute capabilities growth, communication overheads became the main bottleneck of distributed training.&lt;/p&gt;

&lt;p&gt;The goal of this section is to focus on strategies for reducing it. 
In &lt;a href=&quot;#Figure4&quot;&gt;Figure 4.&lt;/a&gt;, we see the different optimization levels and domains that we cover in the following subsections (each in a separate sub-section).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure4&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure4.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 4. Summary of communication optimization levels. The top level refers to communication reduction and scheduling, and the lower level to network traffic.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h3 id=&quot;communication-rounds&quot;&gt;Communication Rounds&lt;/h3&gt;

&lt;p&gt;Training via minibatch SGD consists of multiple epochs (&lt;em&gt;epoch&lt;/em&gt; is an entire pass over the dataset) and iterations (iteration is one SGD round w/ one minibatch, see &lt;a href=&quot;#alg1&quot;&gt;Alg. 1.&lt;/a&gt;). In distributed training algorithms, data is often exchanged at the end of an iteration (e.g., sending gradients, sending model parameters, etc…). Therefore reducing the number of iterations means cutting down the communication rounds.&lt;/p&gt;

&lt;p&gt;The two straightforward ways to reduce the total number of iterations - 1. increasing the batch size and 2. reducing the frequency of exchanging gradients and/or parameters.&lt;/p&gt;

&lt;h4 id=&quot;batch-size&quot;&gt;Batch Size&lt;/h4&gt;

&lt;p&gt;The batch size is a hyper-parameter that controls the amount of data read before each iteration.&lt;/p&gt;

&lt;p&gt;Selecting a too small mini-batch size (see Section A in &lt;a href=&quot;#Figure5&quot;&gt;Figure 5.&lt;/a&gt;) can result in both:&lt;/p&gt;
&lt;ol&gt;
  &lt;li&gt;reduced utilization of the hardware during training (as the max concurrency level isn’t reached), as seen by the Performance line; and&lt;/li&gt;
  &lt;li&gt;statistical inaccuracy due to the variances introduced into the gradients (i.e., the randomly selected samples might not represent the dataset accurately) as seen by the Validation Error line.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;While selecting a large batch size directly answers the concurrency aspect (see Performance line, Section C in &lt;a href=&quot;#Figure5&quot;&gt;Figure 5.&lt;/a&gt;), it might hurt the model accuracy (see Validation Error line) by hurting the optimizations used during training. This can be addressed by several techniques, such as adjusting the learning rate to be a function of the minibatch size, and more (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/corr/GoyalDGNWKTJH17&quot;&gt;[14]&lt;/a&gt;, and Section 3 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;). Note that an earlier paper &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/iclr/KeskarMNST17&quot;&gt;[15]&lt;/a&gt; claimed that the reason for reduced model accuracy with big minibatch sizes is a convergence to a sharp minimum vs. flat minimum with smaller sizes.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure5&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure5.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 5. Performance and accuracy of minibatch SGD after a fixed number of epochs.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/Ben-NunH19&quot;&gt;[1]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;h4 id=&quot;periodic-communication&quot;&gt;Periodic Communication&lt;/h4&gt;

&lt;p&gt;As we mentioned before, our goal is to reduce the number of communication rounds, i.e., occurrences at which participants send/receive model parameters and/or gradients.&lt;/p&gt;

&lt;p&gt;The idea here is that instead of sending the parameters and gradients after each iteration (see (a) in &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;), the average of multiple iterations will be sent after those iterations (e.g., (b) in &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;). The extreme case is a one-shot training at which nothing is sent until all iterations are done (i.e., (c) in  &lt;a href=&quot;#Figure6&quot;&gt;Figure 6.&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The tradeoff of averaging is between communication (that we reduce) and accuracy (that we lose), yet studies showed multiple approaches for averaging that can reduce communication costs while keeping the desired accuracy levels (see Section 3.1.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;).&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure6&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure6.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 6. Different model averaging approached (from no averaging in (a) to the maximum averaging in (c)). A green block indicates computation and a yellow block corresponds to communication.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Another aspect of communication rounds is their typical bursty behavior - in some algorithms (e.g., synchronous approaches), participants send their updates around the same time, leading to unbalanced network usage (in time and locations). In addition, message prioritization can help in some cases in order to meet bandwidth limitations, e.g., depend on the model state (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/mlsys/HashemiJC19&quot;&gt;[16]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/cloud/WeiDQHCGGGX15&quot;&gt;[17]&lt;/a&gt;). Note that these algorithms aren’t implemented yet in open source DL frameworks.&lt;/p&gt;

&lt;h3 id=&quot;gradient-compression&quot;&gt;Gradient Compression&lt;/h3&gt;

&lt;p&gt;See Section &lt;a href=&quot;#parameter-and-gradient-compression&quot;&gt;Parameter and Gradient Compression&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;overlap&quot;&gt;Overlap&lt;/h3&gt;

&lt;p&gt;The idea here is to hide the communication overhead by performing the transmission during computation.&lt;/p&gt;

&lt;p&gt;One example is performing transmission of a layer during the computations of its preceding layer. In case of high latency, there are optimizations such as sending merged gradient messages (of different layers) that can reduce the amount of data sent. In addition,&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot; Figure7&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure7.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 7. One possible case of hiding the communication time overhead behind the backpropagation. In this case, each layer gradients are transmitted as soon as they are ready.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Another example is performing transmission of the weights while the current iteration is still running (while balancing training speed and accuracy). For more details on this approach of hybrid SSGD and ASGD, see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/taco/XuDZXL20&quot;&gt;[18]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;logical-architecture&quot;&gt;Logical Architecture&lt;/h3&gt;

&lt;p&gt;For discussion about topology, see &lt;a href=&quot;#centralization&quot;&gt;Centralization&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;messaging-library&quot;&gt;Messaging Library&lt;/h3&gt;

&lt;p&gt;According to &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;, most of the parameter servers are using either gRPC or ZeroMQ (see here for a nice comparison  &lt;a class=&quot;citation&quot; href=&quot;#grpc-and-zeromq-comparison&quot;&gt;[19]&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Many message-level libraries are implementing Allreduce algorithms efficiently, such as MPI, Gloo, Baidu Allreduce, Aluminum, and BlueConnect (the latter seems to be more efficient than Gloo and Baidu Allreduce).&lt;/p&gt;

&lt;p&gt;For more info, see Section 4.2 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;protocols&quot;&gt;Protocols&lt;/h3&gt;

&lt;p&gt;At this level, improving the communication can be done by reducing the cost of copying data among machines, by moving away from the expensive TCP/IP protocol over sockets (Figure (a) in &lt;a href=&quot;#Figure8&quot;&gt;Figure 8.&lt;/a&gt;, to IPoIB (internet over InfiniBand) or RDMA (Figures (b) and (c) in &lt;a href=&quot;#Figure8&quot;&gt;Figure 8.&lt;/a&gt;)&lt;/p&gt;

&lt;p&gt;RDMA allows one machine to read/write directly from/to the memory of another device without going through the OS (middle layer in the figures). This enables high performance and low latency networking. Further optimizations exist, such as letting GPUs communicate directly over RDMA or doing an adaptive RDMA by dynamically adjusting gRPC message sizes.&lt;/p&gt;

&lt;figure class=&quot;&quot; id=&quot;Figure8&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/Figure8.png&quot; alt=&quot;&quot; /&gt;&lt;figcaption&gt;
      Figure 8. Comparison of TCP/IP Socket, IPoIB, and RDMA.
Figure adapted from &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/figcaption&gt;&lt;/figure&gt;

&lt;p&gt;Experiments show that using RDMA or IPoIB can significantly speed up training. Some even show near-linear speedup with RDMA when scaling training on 100 GPUs.&lt;/p&gt;

&lt;p&gt;For more info, See Section 4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/jpdc/OuyangDXX21&quot;&gt;[12]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;hardware-level-optimizations&quot;&gt;Hardware Level Optimizations&lt;/h2&gt;

&lt;h3 id=&quot;gpu-vs-cpu&quot;&gt;GPU vs CPU&lt;/h3&gt;

&lt;p&gt;The performance benefits of GPU compared to CPU depend on many factors, such as whether the job is processing-bound or memory-bound, the efficiency of the implementation, and the hardware characteristics.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/isca/LeeKCDKNSSCHSD10&quot;&gt;[20]&lt;/a&gt; Intel show how the actual gap between GPU and CPU can be only 2.5x in favor of GPU when using the proper optimizations and HW characteristics. In that paper, they cover the following items:&lt;/p&gt;

    &lt;ol&gt;
      &lt;li&gt;CPU optimizations - multithreading, cache blocking, and reorganization of memory accesses for SIMD-ification.&lt;/li&gt;
      &lt;li&gt;GPU optimizations - minimizing global synchronization and using local shared buffers.&lt;/li&gt;
      &lt;li&gt;HW architecture guidelines - high compute flops and memory bandwidth, large caches, gather/scatter support, efficient synchronization and cache coherence, and fixed functional units.&lt;/li&gt;
    &lt;/ol&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Some systems use CPUs and GPUs for training, and solely CPUs for inference - since GPUs are great for bandwidth but not for latency (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/hpca/HazelwoodBBCDDF18&quot;&gt;[21]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/pvldb/ZouJLGWX14&quot;&gt;[22]&lt;/a&gt;). 
  Other relies solely on CPUs for training due to the abundance of available resources (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nips/DeanCMCDLMRSTYN12&quot;&gt;[23]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt;)&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;There are some interesting optimizations in these papers, e.g., in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt; they partition the DL model layers to fit L3 cache, which shows to improve performance.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;multi-gpu-execution&quot;&gt;Multi-GPU Execution&lt;/h3&gt;

&lt;p&gt;Gaining performance from multi-GPU scaling isn’t trivial, and understanding the performance impact of GPUs interconnections is essential.&lt;/p&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/tpds/LiSCLLTB20&quot;&gt;[25]&lt;/a&gt; they perform an evaluation of latest GPU interconnections w.r.t. raw startup latency, sustainable uni/bi-directional bandwidth, network topology, communication efficiency, routing, and NUMA (non uniform memory access) effects, under two communication patterns - Peer-to-Peer, and Collective.&lt;/p&gt;

&lt;p&gt;They identified several items, e.g., an “anti-locality” pattern (in some NUMA scenarios) at which nearby memory access presents lower performance than remote memory access, and suggested that the root cause is in the unbalanced physical signal paths (e.g., in PCIe-switch chipsets they used).&lt;/p&gt;

&lt;h3 id=&quot;switch-optimizations&quot;&gt;Switch Optimizations&lt;/h3&gt;

&lt;p&gt;In &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/nsdi/SapioC0NKKKMPR21&quot;&gt;[26]&lt;/a&gt; they show how to ease network bottleneck by performing the aggregation part of training on programmable switches (they focus on a single-rack).&lt;/p&gt;

&lt;p&gt;Although these aggregations can be done on the PS side (when using the PS approach), there’s a cost of required computation resources and network bandwidth.&lt;/p&gt;

&lt;h3 id=&quot;cnn-optimizations&quot;&gt;CNN Optimizations&lt;/h3&gt;

&lt;h4 id=&quot;layers-fusion&quot;&gt;Layers Fusion&lt;/h4&gt;

&lt;p&gt;Layer fusion is a technique for accelerating CNN by &lt;em&gt;fusing&lt;/em&gt; the processing of multiple CNN layers. 
Generally, instead of waiting for each layer processing to complete (which results in off-chip memory consumption for the large intermediate states between layers), the input-data processing order is altered to leverage caching of intermediate states of adjacents layers (see  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/micro/AlwaniCFM16&quot;&gt;[27]&lt;/a&gt;).&lt;/p&gt;

&lt;h4 id=&quot;flexible-buffering&quot;&gt;Flexible Buffering&lt;/h4&gt;

&lt;p&gt;The same set of weights is used in all image evaluations in CNN networks.&lt;/p&gt;

&lt;p&gt;Accelerators leverage this by fetching batches of images at a time into on-chip memory, bringing blocks of weights to the on-chip memory, and then reusing the same weight block over the batched images for layer evaluation.&lt;/p&gt;

&lt;p&gt;If we consider a given on-chip storage budget, we see a tradeoff between the batch size (i.e., the number of images) and the amount of data per image stored on-chip.&lt;/p&gt;

&lt;p&gt;This means that by increasing the batch size, we reduce the number of times we need to fetch weight blocks into memory (i.e., reducing the weights bandwidth) but pay by increasing the required bandwidth for transferring data (as we store less per image on-chip).&lt;/p&gt;

&lt;p&gt;In  &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/fccm/ShenFM17&quot;&gt;[28]&lt;/a&gt; they propose a flexible accelerator design that sets the batch size after considering optimal memory bandwidth.&lt;/p&gt;

&lt;h3 id=&quot;other-optimizations&quot;&gt;Other Optimizations&lt;/h3&gt;

&lt;p&gt;For a discussion about other type of hardware approaches (e.g., FPGAs, TPUs) see Section 3.1.1 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;fault-tolerance&quot;&gt;Fault Tolerance&lt;/h2&gt;

&lt;p&gt;What does it mean for an algorithm to be fault-tolerant? The answer depends on the type of failure, the number of failures, and many other dimensions.&lt;/p&gt;

&lt;p&gt;This topic is beyond the scope of our survey. Still, given the nature of distributed environments in which failures are inevitable (e.g., restarting a server as part of continuous integration), we will briefly overview a few distributed training systems and their approaches to dealing with failures.&lt;/p&gt;

&lt;p&gt;Note that it’s crucial to address failures when building a training service - given that the service time (i.e., training time) can be hours, days, or even weeks. In other words, IMHO, asking clients to re-train their model due to a faulty server is not the best design choice.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Project Adam (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;[24]&lt;/a&gt;):
    &lt;ul&gt;
      &lt;li&gt;Impact of slow machines:
        &lt;ul&gt;
          &lt;li&gt;Single sample processing speed when using partitioned model approach depends on the slowest participant. To mitigate this problem, they allowed threads on each participant to process multiple samples in parallel.&lt;/li&gt;
          &lt;li&gt;The slowest participant delays the end time of each epoch, as all training samples are required to compute model prediction error on the validation data set (which determines if another epoch is needed). They addressed this issue by ending epochs after a fraction of the samples (e.g., 75%) are completely processed and shuffling the samples order before each epoch to ensure coverage.&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;Each parameter shard (a 1MB block of parameters) has three copies stored on a different parameter server. A primary-backup scheme is used among those three servers (i.e., a primary that serves the parameters and two secondaries that maintain copies).&lt;/li&gt;
      &lt;li&gt;Clients communicate with a fault-tolerant coordination service to find the parameter server that holds their required parameter shard. This coordination service is responsible for sharding the model and for the assignments of PS servers (generally, it uses Paxos). A lease mechanism is used for detecting faulty servers.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;GeePS and Poseidon (see &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/eurosys/CuiZGGX16&quot;&gt;[29]&lt;/a&gt;, and &lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/usenix/ZhangZXDHLHWXX17&quot;&gt;[30]&lt;/a&gt;):
    &lt;ul&gt;
      &lt;li&gt;In these systems, the parameter server continuously checkpoints the current states in persistent storage.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;from-ideas-to-practice&quot;&gt;From Ideas to Practice&lt;/h2&gt;

&lt;p&gt;This section gives an overview of the leading frameworks and libraries available today that supports distributed training.&lt;/p&gt;

&lt;p&gt;In the following list, we mention for each framework its name, APIs, and supported algorithms:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Chainer (&lt;a class=&quot;citation&quot; href=&quot;#Chainer&quot;&gt;[31]&lt;/a&gt;) | APIs: Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Decentralized only&lt;/li&gt;
      &lt;li&gt;Synchronous only&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;DL4j (&lt;a class=&quot;citation&quot; href=&quot;#DL4J&quot;&gt;[32]&lt;/a&gt;) | APIs: Java | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Modified 1-bit gradient quantization by Strom supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Keras (&lt;a class=&quot;citation&quot; href=&quot;#Keras&quot;&gt;[33]&lt;/a&gt;) | APIs: DL4j, TensorFlow | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Model quantization supported&lt;/li&gt;
      &lt;li&gt;Higher-level concepts must be implemented in the DL framework that employs Keras&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;MXNet (&lt;a class=&quot;citation&quot; href=&quot;#MXNet&quot;&gt;[34]&lt;/a&gt; and &lt;a class=&quot;citation&quot; href=&quot;#MXNet-with-Horovod&quot;&gt;[35]&lt;/a&gt;) | APIs: C++, Go, Java-Script, Matlab, Python, Scala, Wolfram, Julia, Perl, R | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization supported&lt;/li&gt;
      &lt;li&gt;2-bit gradient quantization with error-feedback supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;PyTorch (See Section &lt;a href=&quot;#pytorch&quot;&gt;PyTorch&lt;/a&gt;) | APIs: C++, Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization is supported in beta&lt;/li&gt;
      &lt;li&gt;Gradient quantization is supported in beta&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;SINGA (&lt;a class=&quot;citation&quot; href=&quot;#Signa&quot;&gt;[36]&lt;/a&gt;) | APIs: C++, Python | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization not supported&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;TensorFlow (&lt;a class=&quot;citation&quot; href=&quot;#DBLP:conf/osdi/AbadiBCCDDDGIIK16&quot;&gt;[37]&lt;/a&gt;, and see Section &lt;a href=&quot;#tensorflow--w-keras&quot;&gt;TensorFlow&lt;/a&gt;) | APIs: C++, Go, Java, Java-Script, Python, Swift | Supported Algorithms:
    &lt;ul&gt;
      &lt;li&gt;Centralized and decentralized&lt;/li&gt;
      &lt;li&gt;Synchronous and asynchronous&lt;/li&gt;
      &lt;li&gt;Model quantization supported (with Keras)&lt;/li&gt;
      &lt;li&gt;Gradient quantization not supported (but on roadmap..)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;We now focus on two frameworks - TensorFlow and PyTorch:&lt;/p&gt;

&lt;h3 id=&quot;tensorflow--w-keras&quot;&gt;TensorFlow ( w/ Keras)&lt;/h3&gt;

&lt;p&gt;TensorFlow natively supports distributed and parallel training (see &lt;a class=&quot;citation&quot; href=&quot;#Distributed-training-with-TensorFlow&quot;&gt;[38, 39, 40, 41, 42, 43]&lt;/a&gt;). In particular, it supports both model parallelism and data parallelism. In data parallelism, the centralized approach via parameter servers is supported, using either asynchronous or synchronous training. Trained models can be quantized using TensorFlow Lite (see &lt;a class=&quot;citation&quot; href=&quot;#Post-training-quantization&quot;&gt;[44]&lt;/a&gt;). Currently, there is no native support for gradient quantization &lt;a class=&quot;citation&quot; href=&quot;#Quantization-aware-training&quot;&gt;[45]&lt;/a&gt;, but this can be done via Keras &lt;a class=&quot;citation&quot; href=&quot;#Quantization-aware-training-in-Keras-example&quot;&gt;[46]&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;pytorch&quot;&gt;PyTorch&lt;/h3&gt;

&lt;p&gt;PyTorch has native support for distributed, data-parallel training, as well as model-parallel training (see &lt;a class=&quot;citation&quot; href=&quot;#PYTORCH-distributed&quot;&gt;[47, 48]&lt;/a&gt;). For data-parallel training, PyTorch implements the decentralized architecture and supports synchronous and asynchronous training. PyTorch supports model quantization via the QNNPACK library (see &lt;a class=&quot;citation&quot; href=&quot;#Quantized&quot;&gt;[49]&lt;/a&gt;). Gradient quantization is supported (see &lt;a class=&quot;citation&quot; href=&quot;#Pytorch-QUANTIZATION&quot;&gt;[50]&lt;/a&gt;).&lt;/p&gt;

&lt;h2 id=&quot;building-a-training-service---related-topics&quot;&gt;Building a Training Service - Related Topics&lt;/h2&gt;

&lt;p&gt;Topics that weren’t covered in this survey but are related to building &amp;amp; maintaining a training service -&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Scheduling and Elasticity (see Section 3.4 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Data Management (see Section 3.5 in &lt;a class=&quot;citation&quot; href=&quot;#DBLP:journals/csur/MayerJ20&quot;&gt;[2]&lt;/a&gt;)&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;epilogue&quot;&gt;Epilogue&lt;/h2&gt;

&lt;p&gt;I hope you found this summary helpful and that you feel that you know the challenges in distributed training.&lt;/p&gt;

&lt;figure class=&quot;&quot;&gt;
  &lt;img src=&quot;/assets/images/DNN-Training-Images/I-Know-DT.png&quot; alt=&quot;&quot; /&gt;&lt;/figure&gt;

&lt;p&gt;I tried to keep it short and mention just the essential parts required for developing an intuition about the topic.
If you think something is missing, incorrect, or just can be improved, please feel free to reach out. I’ll appreciate any feedback. Thanks!&lt;/p&gt;

&lt;h2 id=&quot;refrences&quot;&gt;Refrences&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;DBLP:journals/csur/Ben-NunH19&quot;&gt;Tal Ben-Nun and Torsten Hoefler. &lt;b&gt;Demystifying Parallel and Distributed Deep Learning: An In-depth Concurrency
               Analysis&lt;/b&gt;. &lt;i&gt;ACM Comput. Surv.&lt;/i&gt;, 52(4) :65:1–65:43, 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/csur/MayerJ20&quot;&gt;Ruben Mayer and Hans-Arno Jacobsen. &lt;b&gt;Scalable Deep Learning on Distributed Infrastructures: Challenges,
               Techniques, and Tools&lt;/b&gt;. &lt;i&gt;ACM Comput. Surv.&lt;/i&gt;, 53(1) :3:1–3:37, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/icml/MirhoseiniPLSLZ17&quot;&gt;Azalia Mirhoseini, Hieu Pham, Quoc V. Le, Benoit Steiner, Rasmus Larsen, Yuefeng Zhou, Naveen Kumar, Mohammad Norouzi, Samy Bengio, and Jeff Dean. &lt;b&gt;Device Placement Optimization with Reinforcement Learning&lt;/b&gt;. In: Precup, D. and Teh, Y.W. (eds.) &lt;i&gt;Proceedings of the 34th International Conference on Machine Learning,
               ICML 2017, Sydney, NSW, Australia, 6-11 August 2017&lt;/i&gt;, pp. 2430–2439, PMLR, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/LuoNCPK18&quot;&gt;Liang Luo, Jacob Nelson, Luis Ceze, Amar Phanishayee, and Arvind Krishnamurthy. &lt;b&gt;Parameter Hub: a Rack-Scale Parameter Server for Distributed Deep
               Neural Network Training&lt;/b&gt;. In: &lt;i&gt;Proceedings of the ACM Symposium on Cloud Computing, SoCC 2018,
               Carlsbad, CA, USA, October 11-13, 2018&lt;/i&gt;, pp. 41–54, ACM, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/HoCCLKGGGX13&quot;&gt;Qirong Ho, James Cipar, Henggang Cui, Seunghak Lee, Jin Kyu Kim, Phillip B. Gibbons, Garth A. Gibson, Gregory R. Ganger, and Eric P. Xing. &lt;b&gt;More Effective Distributed ML via a Stale Synchronous Parallel Parameter
               Server&lt;/b&gt;. In: Burges, C.J.C., Bottou, L., Ghahramani, Z., and Weinberger, K.Q. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 26: 27th Annual
               Conference on Neural Information Processing Systems 2013. Proceedings
               of a meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States&lt;/i&gt;, pp. 1223–1231 2013.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/abs-1902-06855&quot;&gt;Peng Sun, Wansen Feng, Ruobing Han, Shengen Yan, and Yonggang Wen. &lt;b&gt;Optimizing Network Performance for Distributed DNN Training on GPU
               Clusters: ImageNet/AlexNet Training in 1.5 Minutes&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1902.06855 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/jpdc/PatarasukY09&quot;&gt;Pitch Patarasuk and Xin Yuan. &lt;b&gt;Bandwidth optimal all-reduce algorithms for clusters of workstations&lt;/b&gt;. &lt;i&gt;J. Parallel Distributed Comput.&lt;/i&gt;, 69(2) :117–124, 2009.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/abs-1807-11205&quot;&gt;Xianyan Jia, Shutao Song, Wei He, Yangzihao Wang, Haidong Rong, Feihu Zhou, Liqiang Xie, Zhenyu Guo, Yuanzhou Yang, Liwei Yu, Tiegang Chen, Guangxiao Hu, Shaohuai Shi, and Xiaowen Chu. &lt;b&gt;Highly Scalable Deep Learning Training System with Mixed-Precision:
               Training ImageNet in Four Minutes&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1807.11205 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/apsys/WangZGZ14&quot;&gt;Minjie Wang, Hucheng Zhou, Minyi Guo, and Zheng Zhang. &lt;b&gt;A scalable and topology configurable protocol for distributed parameter
               synchronization&lt;/b&gt;. In: &lt;i&gt;Asia-Pacific Workshop on Systems, APSys’14, Beijing, China, June 25-26,
               2014&lt;/i&gt;, pp. 13:1–13:7, ACM, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/WatcharapichatM16&quot;&gt;Pijika Watcharapichat, Victoria Lopez Morales, Raul Castro Fernandez, and Peter R. Pietzuch. &lt;b&gt;Ako: Decentralised Deep Learning with Partial Gradient Exchange&lt;/b&gt;. In: Aguilera, M.K., Cooper, B., and Diao, Y. (eds.) &lt;i&gt;Proceedings of the Seventh ACM Symposium on Cloud Computing, Santa
               Clara, CA, USA, October 5-7, 2016&lt;/i&gt;, pp. 84–97, ACM, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/LianZZHZL17&quot;&gt;Xiangru Lian, Ce Zhang, Huan Zhang, Cho-Jui Hsieh, Wei Zhang, and Ji Liu. &lt;b&gt;Can Decentralized Algorithms Outperform Centralized Algorithms? A
               Case Study for Decentralized Parallel Stochastic Gradient Descent&lt;/b&gt;. In: Guyon, I., Luxburg, U. von, Bengio, S., Wallach, H.M., Fergus, R., Vishwanathan, S.V.N., and Garnett, R. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 30: Annual Conference
               on Neural Information Processing Systems 2017, December 4-9, 2017,
               Long Beach, CA, USA&lt;/i&gt;, pp. 5330–5340 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/jpdc/OuyangDXX21&quot;&gt;Shuo Ouyang, Dezun Dong, Yemao Xu, and Liquan Xiao. &lt;b&gt;Communication optimization strategies for distributed deep neural
               network training: A survey&lt;/b&gt;. &lt;i&gt;J. Parallel Distributed Comput.&lt;/i&gt;, 149 :52–65, 2021.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;10754/662495&quot;&gt;Hang Xu, Chen-Yu Ho, Ahmed M. Abdelmoniem, Aritra Dutta, El Houcine Bergou, Konstantinos Karatsenidis, Marco Canini, and Panos Kalnis. &lt;b&gt;Compressed Communication for Distributed Deep Learning: Survey and Quantitative Evaluation&lt;/b&gt;. 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/corr/GoyalDGNWKTJH17&quot;&gt;Priya Goyal, Piotr Dollár, Ross B. Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. &lt;b&gt;Accurate, Large Minibatch SGD: Training ImageNet in 1 Hour&lt;/b&gt;. &lt;i&gt;CoRR&lt;/i&gt;, abs/1706.02677 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/iclr/KeskarMNST17&quot;&gt;Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, and Ping Tak Peter Tang. &lt;b&gt;On Large-Batch Training for Deep Learning: Generalization Gap and
               Sharp Minima&lt;/b&gt;. In: &lt;i&gt;5th International Conference on Learning Representations, ICLR 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings&lt;/i&gt;, OpenReview.net, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/mlsys/HashemiJC19&quot;&gt;Sayed Hadi Hashemi, Sangeetha Abdu Jyothi, and Roy H. Campbell. &lt;b&gt;TicTac: Accelerating Distributed Deep Learning with Communication
               Scheduling&lt;/b&gt;. In: Talwalkar, A., Smith, V., and Zaharia, M. (eds.) &lt;i&gt;Proceedings of Machine Learning and Systems 2019, MLSys 2019, Stanford,
               CA, USA, March 31 - April 2, 2019&lt;/i&gt;, mlsys.org, 2019.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/cloud/WeiDQHCGGGX15&quot;&gt;Jinliang Wei, Wei Dai, Aurick Qiao, Qirong Ho, Henggang Cui, Gregory R. Ganger, Phillip B. Gibbons, Garth A. Gibson, and Eric P. Xing. &lt;b&gt;Managed communication and consistency for fast data-parallel iterative
               analytics&lt;/b&gt;. In: Ghandeharizadeh, S., Barahmand, S., Balazinska, M., and Freedman, M.J. (eds.) &lt;i&gt;Proceedings of the Sixth ACM Symposium on Cloud Computing, SoCC
               2015, Kohala Coast, Hawaii, USA, August 27-29, 2015&lt;/i&gt;, pp. 381–394, ACM, 2015.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/taco/XuDZXL20&quot;&gt;Yemao Xu, Dezun Dong, Yawei Zhao, Weixia Xu, and Xiangke Liao. &lt;b&gt;OD-SGD: One-Step Delay Stochastic Gradient Descent for Distributed
               Training&lt;/b&gt;. &lt;i&gt;ACM Trans. Archit. Code Optim.&lt;/i&gt;, 17(4) :30:1–30:26, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;grpc-and-zeromq-comparison&quot;&gt;&lt;b&gt;grpc and zeromq comparsion&lt;/b&gt;, https://stackoverflow.com/questions/39350681/grpc-and-zeromq-comparsion, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/isca/LeeKCDKNSSCHSD10&quot;&gt;Victor W. Lee, Changkyu Kim, Jatin Chhugani, Michael Deisher, Daehyun Kim, Anthony D. Nguyen, Nadathur Satish, Mikhail Smelyanskiy, Srinivas Chennupaty, Per Hammarlund, Ronak Singhal, and Pradeep Dubey. &lt;b&gt;Debunking the 100X GPU vs. CPU myth: an evaluation of throughput
               computing on CPU and GPU&lt;/b&gt;. In: Seznec, A., Weiser, U.C., and Ronen, R. (eds.) &lt;i&gt;37th International Symposium on Computer Architecture (ISCA 2010),
               June 19-23, 2010, Saint-Malo, France&lt;/i&gt;, pp. 451–460, ACM, 2010.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/hpca/HazelwoodBBCDDF18&quot;&gt;Kim M. Hazelwood, Sarah Bird, David M. Brooks, Soumith Chintala, Utku Diril, Dmytro Dzhulgakov, Mohamed Fawzy, Bill Jia, Yangqing Jia, Aditya Kalro, James Law, Kevin Lee, Jason Lu, Pieter Noordhuis, Misha Smelyanskiy, Liang Xiong, and Xiaodong Wang. &lt;b&gt;Applied Machine Learning at Facebook: A Datacenter Infrastructure
               Perspective&lt;/b&gt;. In: &lt;i&gt;IEEE International Symposium on High Performance Computer Architecture,
               HPCA 2018, Vienna, Austria, February 24-28, 2018&lt;/i&gt;, pp. 620–629, IEEE Computer Society, 2018.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/pvldb/ZouJLGWX14&quot;&gt;Yongqiang Zou, Xing Jin, Yi Li, Zhimao Guo, Eryu Wang, and Bin Xiao. &lt;b&gt;Mariana: Tencent Deep Learning Platform and its Applications&lt;/b&gt;. &lt;i&gt;Proc. VLDB Endow.&lt;/i&gt;, 7(13) :1772–1777, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nips/DeanCMCDLMRSTYN12&quot;&gt;Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Quoc V. Le, Mark Z. Mao, Marc’Aurelio Ranzato, Andrew W. Senior, Paul A. Tucker, Ke Yang, and Andrew Y. Ng. &lt;b&gt;Large Scale Distributed Deep Networks&lt;/b&gt;. In: Bartlett, P.L., Pereira, F.C.N., Burges, C.J.C., Bottou, L., and Weinberger, K.Q. (eds.) &lt;i&gt;Advances in Neural Information Processing Systems 25: 26th Annual
               Conference on Neural Information Processing Systems 2012. Proceedings
               of a meeting held December 3-6, 2012, Lake Tahoe, Nevada, United States&lt;/i&gt;, pp. 1232–1240 2012.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/osdi/ChilimbiSAK14&quot;&gt;Trishul M. Chilimbi, Yutaka Suzue, Johnson Apacible, and Karthik Kalyanaraman. &lt;b&gt;Project Adam: Building an Efficient and Scalable Deep Learning Training
               System&lt;/b&gt;. In: Flinn, J. and Levy, H. (eds.) &lt;i&gt;11th USENIX Symposium on Operating Systems Design and Implementation,
               OSDI ’14, Broomfield, CO, USA, October 6-8, 2014&lt;/i&gt;, pp. 571–582, USENIX Association, 2014.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:journals/tpds/LiSCLLTB20&quot;&gt;Ang Li, Shuaiwen Leon Song, Jieyang Chen, Jiajia Li, Xu Liu, Nathan R. Tallent, and Kevin J. Barker. &lt;b&gt;Evaluating Modern GPU Interconnect: PCIe, NVLink, NV-SLI, NVSwitch
               and GPUDirect&lt;/b&gt;. &lt;i&gt;IEEE Trans. Parallel Distributed Syst.&lt;/i&gt;, 31(1) :94–110, 2020.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/nsdi/SapioC0NKKKMPR21&quot;&gt;Amedeo Sapio, Marco Canini, Chen-Yu Ho, Jacob Nelson, Panos Kalnis, Changhoon Kim, Arvind Krishnamurthy, Masoud Moshref, Dan R. K. Ports, and Peter Richtárik. &lt;b&gt;Scaling Distributed Machine Learning with In-Network Aggregation&lt;/b&gt;. In: Mickens, J. and Teixeira, R. (eds.) &lt;i&gt;18th USENIX Symposium on Networked Systems Design and Implementation,
               NSDI 2021, April 12-14, 2021&lt;/i&gt;, pp. 785–808, USENIX Association, 2021.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/micro/AlwaniCFM16&quot;&gt;Manoj Alwani, Han Chen, Michael Ferdman, and Peter A. Milder. &lt;b&gt;Fused-layer CNN accelerators&lt;/b&gt;. In: &lt;i&gt;49th Annual IEEE/ACM International Symposium on Microarchitecture,
               MICRO 2016, Taipei, Taiwan, October 15-19, 2016&lt;/i&gt;, pp. 22:1–22:12, IEEE Computer Society, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/fccm/ShenFM17&quot;&gt;Yongming Shen, Michael Ferdman, and Peter A. Milder. &lt;b&gt;Escher: A CNN Accelerator with Flexible Buffering to Minimize
               Off-Chip Transfer&lt;/b&gt;. In: &lt;i&gt;25th IEEE Annual International Symposium on Field-Programmable Custom
               Computing Machines, FCCM 2017, Napa, CA, USA, April 30 - May 2,
               2017&lt;/i&gt;, pp. 93–100, IEEE Computer Society, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/eurosys/CuiZGGX16&quot;&gt;Henggang Cui, Hao Zhang, Gregory R. Ganger, Phillip B. Gibbons, and Eric P. Xing. &lt;b&gt;GeePS: scalable deep learning on distributed GPUs with a GPU-specialized
               parameter server&lt;/b&gt;. In: Cadar, C., Pietzuch, P.R., Keeton, K., and Rodrigues, R. (eds.) &lt;i&gt;Proceedings of the Eleventh European Conference on Computer Systems,
               EuroSys 2016, London, United Kingdom, April 18-21, 2016&lt;/i&gt;, pp. 4:1–4:16, ACM, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/usenix/ZhangZXDHLHWXX17&quot;&gt;Hao Zhang, Zeyu Zheng, Shizhen Xu, Wei Dai, Qirong Ho, Xiaodan Liang, Zhiting Hu, Jinliang Wei, Pengtao Xie, and Eric P. Xing. &lt;b&gt;Poseidon: An Efficient Communication Architecture for Distributed
               Deep Learning on GPU Clusters&lt;/b&gt;. In: Silva, D.D. and Ford, B. (eds.) &lt;i&gt;2017 USENIX Annual Technical Conference, USENIX ATC 2017, Santa
               Clara, CA, USA, July 12-14, 2017&lt;/i&gt;, pp. 181–193, USENIX Association, 2017.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Chainer&quot;&gt;&lt;b&gt;Chainer&lt;/b&gt;, https://docs.chainer.org/en/stable/chainermn/index.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DL4J&quot;&gt;&lt;b&gt;DL4J’s Distributed Training Implementations&lt;/b&gt;, https://deeplearning4j.konduit.ai/spark/tutorials/dl4j-on-spark-quickstart#dl4js-distributed-training-implementations, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Keras&quot;&gt;&lt;b&gt;Keras - Multi-GPU and distributed training&lt;/b&gt;, https://keras.io/guides/distributed_training/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;MXNet&quot;&gt;&lt;b&gt;MXNet&lt;/b&gt;, https://mxnet.apache.org/versions/1.8.0/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;MXNet-with-Horovod&quot;&gt;&lt;b&gt;Distributed Training using Apache MXNet with Horovod&lt;/b&gt;, https://medium.com/apache-mxnet/distributed-training-using-apache-mxnet-with-horovod-44f98bf0e7b7, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Signa&quot;&gt;&lt;b&gt;Signa - Distributed Training&lt;/b&gt;, https://singa.apache.org/docs/dist-train/, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;DBLP:conf/osdi/AbadiBCCDDDGIIK16&quot;&gt;Abadi Martı́n, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner, Paul A. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng. &lt;b&gt;TensorFlow: A System for Large-Scale Machine Learning&lt;/b&gt;. In: Keeton, K. and Roscoe, T. (eds.) &lt;i&gt;12th USENIX Symposium on Operating Systems Design and Implementation,
               OSDI 2016, Savannah, GA, USA, November 2-4, 2016&lt;/i&gt;, pp. 265–283, USENIX Association, 2016.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Distributed-training-with-TensorFlow&quot;&gt;&lt;b&gt;Distributed training with TensorFlow&lt;/b&gt;, https://www.tensorflow.org/guide/distributed_training, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;TensorFlow-Optimize-further&quot;&gt;&lt;b&gt;TensorFlow - Optimize further&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/optimize_further, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Distributed-TensorFlow-training&quot;&gt;&lt;b&gt;Distributed TensorFlow training (Google I/O ’18)&lt;/b&gt;, https://www.youtube.com/watch?v=bRMGoPqsn20, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Inside-TensorFlow-tf.data-tf.distribute&quot;&gt;&lt;b&gt;Inside TensorFlow: tf.data + tf.distribute (Dev Summit 2019)&lt;/b&gt;, https://www.youtube.com/watch?v=ZnukSLKEw34, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Inside-TensorFlow-tf.distribute.Strategy&quot;&gt;&lt;b&gt;Inside TensorFlow: tf.distribute.Strategy&lt;/b&gt;, https://www.youtube.com/watch?v=jKV53r9-H14, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Scaling-TensorFlow-2-models-to-multi-worker-GPUs&quot;&gt;&lt;b&gt;Scaling TensorFlow 2 models to multi-worker GPUs (TF Dev Summit ‘20)&lt;/b&gt;, https://www.youtube.com/watch?v=6ovfZW8pepo, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Post-training-quantization&quot;&gt;&lt;b&gt;Post-training quantization&lt;/b&gt;, https://www.tensorflow.org/lite/performance/post_training_quantization, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantization-aware-training&quot;&gt;&lt;b&gt;Quantization aware training&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/quantization/training, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantization-aware-training-in-Keras-example&quot;&gt;&lt;b&gt;Quantization aware training in Keras example&lt;/b&gt;, https://www.tensorflow.org/model_optimization/guide/quantization/training_example, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;PYTORCH-distributed&quot;&gt;&lt;b&gt;PYTORCH DISTRIBUTED OVERVIEW&lt;/b&gt;, https://pytorch.org/tutorials/beginner/dist_overview.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;TORCH.DISTRIBUTED&quot;&gt;&lt;b&gt;DISTRIBUTED COMMUNICATION PACKAGE - TORCH.DISTRIBUTED&lt;/b&gt;, https://pytorch.org/docs/stable/distributed.html?highlight=distributed#basics, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Quantized&quot;&gt;&lt;b&gt;Quantized Neural Networks PACKage&lt;/b&gt;, https://github.com/pytorch/QNNPACK, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;Pytorch-QUANTIZATION&quot;&gt;&lt;b&gt;Pytorch QUANTIZATION&lt;/b&gt;, https://pytorch.org/docs/stable/quantization.html, 2021 Accesssed: 2021-12-20.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html">I love both distributed systems and machine learning. However, when I started grad school, I wasn’t sure which one to choose. In my first semester, the ML 101 course wasn’t taught while parallel computing 101 did, and so I decided based on availability :smile:</summary></entry><entry><title type="html">DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub.</title><link href="https://kfirlevari.github.io/CreatingAWebSite/" rel="alternate" type="text/html" title="DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub." /><published>2021-12-21T00:00:00-08:00</published><updated>2021-12-21T00:00:00-08:00</updated><id>https://kfirlevari.github.io/CreatingAWebSite</id><content type="html" xml:base="https://kfirlevari.github.io/CreatingAWebSite/">&lt;p&gt;In this post, I’ll share how I built this website,
i.e., one hosted in GitHub Pages and powered by Jekyll &amp;amp; Minimal Mistakes.&lt;/p&gt;

&lt;p&gt;I think that it can be done by anyone enthusiastic about learning new technologies (actually, a keyboard, a screen, and a connection to the internet are also required :smile:).&lt;/p&gt;

&lt;p&gt;I assume you have a basic familiarity with git. If you’re unsure how to work with git, you can start &lt;a href=&quot;https://docs.github.com/en/get-started/using-git/about-git&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;And most important, questions and comments are welcome :wink:&lt;/p&gt;

&lt;h2 id=&quot;preparations&quot;&gt;Preparations&lt;/h2&gt;

&lt;h3 id=&quot;step-1-github-pages&quot;&gt;Step 1. GitHub Pages&lt;/h3&gt;

&lt;p&gt;Like I mentioned, the site is hosted in GitHub Pages. What does it mean? see &lt;a href=&quot;https://pages.github.com/&quot;&gt;here&lt;/a&gt;.
A good starting point is to follow the instructions in that link and create your website repository (at &amp;lt;your GitHub username&amp;gt;.github.io).&lt;/p&gt;

&lt;p&gt;Later you’ll place there the content of your website. But, for now, you should only be able to see the “Hello World” page there.&lt;/p&gt;

&lt;p&gt;Let’s also create a local clone of your website repository. E.g., to “/Repos/&amp;lt;your GitHub username&amp;gt;.github.io” directory.&lt;/p&gt;

&lt;h3 id=&quot;step-2-preparations-for-jekyll--minimal-mistakes&quot;&gt;Step 2. Preparations for Jekyll &amp;amp; Minimal Mistakes&lt;/h3&gt;

&lt;p&gt;What’s Jekyll? a quote from &lt;a href=&quot;https://jekyllrb.com/docs/&quot;&gt;here&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;“Jekyll is a static site generator. It takes text written in your favorite markup language and uses layouts to create a static website.”&lt;/p&gt;

&lt;p&gt;So basically, Jekyll can build a website from files that we write in a markup language. What markup language we’ll use? a simple one named &lt;a href=&quot;https://www.markdownguide.org/getting-started/&quot;&gt;Markdown&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;To run Jekyll, we need to prepare our environment. First, you should install the requirements listed &lt;a href=&quot;https://jekyllrb.com/docs/installation/#requirements&quot;&gt;here&lt;/a&gt; (i.e., Ruby, RubyGems, GCC, and Make) if you’re not sure how then follow the installation guide that matches your operating system &lt;a href=&quot;https://jekyllrb.com/docs/installation/#guides&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;What’s Minimal Mistakes? From &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/about/&quot;&gt;here&lt;/a&gt;: “A flexible two-column Jekyll theme”. We’ll see later how to import it.&lt;/p&gt;

&lt;h3 id=&quot;step-3-source-directory-preparations&quot;&gt;Step 3. Source Directory Preparations&lt;/h3&gt;

&lt;p&gt;Jekyll will transform a source directory into a website. To verify the changes that we do in the source directory while working on our website and be resilient to failures (so that we won’t lose the source of our website if our computer crashes), we’ll use online version control.&lt;/p&gt;

&lt;p&gt;It would help to create a new repository for the source directory in this step.
For example - you can create another repository in GitHub. This time, it can be private (unlike &amp;lt;your GitHub username&amp;gt;.github.io that needs to be public).&lt;/p&gt;

&lt;p&gt;After creating the source directory repository, clone it locally (e.g., to “/Repos/source” directory). Then, we populate it in the next step.&lt;/p&gt;

&lt;h3 id=&quot;step-4-importing-minimal-mistakes&quot;&gt;Step 4. Importing Minimal Mistakes&lt;/h3&gt;

&lt;p&gt;Our goal in this step is to have &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/structure/&quot;&gt;these files and folders&lt;/a&gt; in your local source directory.&lt;/p&gt;

&lt;p&gt;One way to get these files is by cloning &lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal Mistakes repository&lt;/a&gt; locally (e.g., to “Repos/minimal-mistakes”), and then manually copying the required files to your “/Repos/source” directory.
After this step, we won’t use “Repos/minimal-mistakes”, so you can delete it.&lt;/p&gt;

&lt;h3 id=&quot;step-5-populate-the-gemfile&quot;&gt;Step 5. Populate the GemFile&lt;/h3&gt;

&lt;p&gt;What is RubyGems (that we installed as part of &lt;a href=&quot;#step-2-preparations-for-jekyll--minimal-mistakes&quot;&gt;Step 2&lt;/a&gt;)? From &lt;a href=&quot;https://en.wikipedia.org/wiki/RubyGems&quot;&gt;here&lt;/a&gt;: “RubyGems is a package manager for the Ruby programming language that provides a standard format for distributing Ruby programs and libraries (in a self-contained format called a \ “gem&quot;), a tool designed to manage the installation of gems easily, and a server for distributing them.”&lt;/p&gt;

&lt;p&gt;We will populate the GemFile in “/Repos/source” with the packages that our website needs.&lt;/p&gt;

&lt;p&gt;Here is the GemFile I’m using:&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;source &quot;https://rubygems.org&quot;

gem &quot;github-pages&quot;, group: :jekyll_plugins
gem &quot;jekyll-include-cache&quot;, group: :jekyll_plugins
gem &apos;jekyll-academicons-svg&apos;
gem &apos;jemoji&apos;
gem &apos;jekyll-scholar&apos;, group: :jekyll_plugins
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;github-pages, and jekyll-include-cache: required for us to be able to run our theme in GitHub Pages, as explained &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/quick-start-guide/#remote-theme-method&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jekyll-academicons-svg: in order to use &lt;a href=&quot;https://jpswalsh.github.io/academicons/&quot;&gt;academicons&lt;/a&gt;. Plugin repo &lt;a href=&quot;https://github.com/sylvainmetayer/jekyll-academicons-svg&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jemoji: because one can’t express oneself online these days without emojis, or can we :thinking:? Plugin repo &lt;a href=&quot;https://github.com/jekyll/jemoji&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;jekyll-scholar: gives you citation superpowers, as explained &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;step-6-install-jekyll-and-bundler&quot;&gt;Step 6. Install Jekyll and Bundler&lt;/h3&gt;

&lt;p&gt;To install Jekyll and &lt;a href=&quot;https://bundler.io/&quot;&gt;Bundler&lt;/a&gt; gems locally, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gem install jekyll bundler&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;We’ll use bundler to install gems and run Jekyll.&lt;/p&gt;

&lt;h2 id=&quot;build-your-homepage&quot;&gt;Build Your Homepage&lt;/h2&gt;

&lt;h3 id=&quot;step-1-set-repository-name&quot;&gt;Step 1. Set Repository Name&lt;/h3&gt;

&lt;p&gt;The config file “_config.yml” contains many knobs. See &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;here&lt;/a&gt; for the full list.&lt;/p&gt;

&lt;p&gt;You don’t have to update all of them right now, but you do need to set some of them -&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Fill your website repository name, i.e, repository : “&amp;lt;your GitHub username&amp;gt;.github.io”&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Add “github: [metadata]” at the end of your config file (based on &lt;a href=&quot;https://github.com/github/pages-gem/issues/399&quot;&gt;this&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Comment out both “theme:” and “remote theme:” lines - we’re using a local copy of Minimal Mistakes, so we don’t need to import anything else.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make sure that you have ‘jekyll-include-cache’ in the plugins array. For example, here is my plugins array (I think you’ll have the first 5):&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;plugins:
  - jekyll-paginate
  - jekyll-sitemap
  - jekyll-gist
  - jekyll-feed
  - jekyll-include-cache
  - jemoji
  - jekyll-scholar
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;step-2-sanity-check&quot;&gt;Step 2. Sanity Check&lt;/h3&gt;

&lt;p&gt;In your source directory, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;and once all went well, run:&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;bundle exec jekyll serve --incremental --verbose&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;You should see your new website at &lt;a href=&quot;http://localhost:4000&quot;&gt;http://localhost:4000&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Note that the optional –incremental flag enables you to test your website while changing it (but it’s experimental, see &lt;a href=&quot;https://jekyllrb.com/docs/configuration/incremental-regeneration/&quot;&gt;here&lt;/a&gt;), and the optional –verbose flag can help you debug issues you have in can Jekyll fails to build your site.&lt;/p&gt;

&lt;h3 id=&quot;step-3-design-your-website&quot;&gt;Step 3. Design Your Website&lt;/h3&gt;

&lt;p&gt;I’m no web designer, so I don’t know the right protocol here :smile:. But let me share a few things that I did and might help you, do them in any order you see fit:&lt;/p&gt;

&lt;h4 id=&quot;config-file&quot;&gt;Config file&lt;/h4&gt;

&lt;p&gt;Look closely at the configurations &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/configuration/&quot;&gt;here&lt;/a&gt;, and modify your _config.yml accordingly.&lt;/p&gt;

&lt;h4 id=&quot;navigation&quot;&gt;Navigation&lt;/h4&gt;

&lt;p&gt;In _data/navigation.yml there’s the main array. Based on it, the links at the top are generated. E.g., my main array is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;main:
  - title: &quot;Blog&quot;
    url: /blog/
  - title: &quot;Projects&quot;
    url: /projects/
  - title: &quot;Publications&quot;
    url: /publications/
  - title: &quot;Past Teaching&quot;
    url: /past-teaching/
  - title: &quot;Personal&quot;
    url: /personal/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;pages-posts-and-index&quot;&gt;Pages, Posts, and Index&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Create a new directory named “_pages” (see also &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/pages/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://jekyllrb.com/docs/pages/&quot;&gt;here&lt;/a&gt;).
You should add a .md file in /_pages for every link that you added in the main array.
E.g., I have /_pages/personal.md, that corresponds to the personal page.&lt;/li&gt;
  &lt;li&gt;If you want a blog, create a new directory named “_posts” (see also &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/posts/&quot;&gt;here&lt;/a&gt;, and &lt;a href=&quot;https://jekyllrb.com/docs/posts/&quot;&gt;here&lt;/a&gt;).
Follow the name format when creating new posts. Note that only posts in the past are visible by default in the blog.&lt;/li&gt;
  &lt;li&gt;Your front page is generated based on the index.html file.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;front-matter-and-layouts&quot;&gt;Front Matter and Layouts&lt;/h4&gt;

&lt;p&gt;At the top of every page, there’s a YAML front matter block. Read about it &lt;a href=&quot;https://jekyllrb.com/docs/front-matter/&quot;&gt;here&lt;/a&gt;.
For example, the front matter of this page is:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: single
title: &quot;DIY - Homepage with Jekyll &amp;amp; Minimal Mistakes in GitHub.&quot;
toc: true
toc_label: &quot;Steps&quot;
toc_icon: &quot;shoe-prints&quot;
toc_sticky: true
---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;(“toc” stands for Table Of Content :wink:)&lt;/p&gt;

&lt;p&gt;You set the page’s layout by changing these values (e.g., the layout of this page is “single”).
For all Minimal Mistakes layouts, see &lt;a href=&quot;https://mmistakes.github.io/minimal-mistakes/docs/layouts/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;We copied Minimal Mistakes source code to our project folder (our source directory). This means that you can modify every template that you want, just be sure that you know what you’re doing :smile:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;E.g., the “single” layout file is found at _layouts/single.html.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;publications&quot;&gt;Publications&lt;/h4&gt;
&lt;p&gt;I followed the instructions in &lt;a href=&quot;https://github.com/inukshuk/jekyll-scholar&quot;&gt;jekyll-scholar&lt;/a&gt; and &lt;a href=&quot;http://pascalpoizat.github.io/blog/posts/2016/02/01/jekyll-and-bibtex/&quot;&gt;pascalpoizat.github.io&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Generally, I did the following:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Created /assets/bibliography directory&lt;/li&gt;
  &lt;li&gt;Placed there a bib style file (*.csl). Chose your favorite, or modify one; there are too many out there. I used &lt;a href=&quot;https://github.com/pascalpoizat/pascalpoizat.github.io/blob/master/src/_bibliography/mystyle.csl&quot;&gt;this style&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;Also placed a references.bib file with all the BibTeX citations.&lt;/li&gt;
  &lt;li&gt;In the config file, create this array:&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;scholar:
  source: /assets/bibliography
  style: /assets/bibliography/mystyle.csl
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Created the following publications.md file in /_pages/ :&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
permalink: /publications/
title: &quot;Publications&quot;
layout: single
classes: wide
header:
  overlay_image: /assets/images/Publications.jpg
  overlay_filter: 0.2
  caption: &quot;[See more photos here](/personal/#photography)&quot;
---

{ % bibliography % }

---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note that the header part relates to the header image, and the “classes” is for setting a widescreen. Also, you shouldn’t have a space between { and %, or % and }.&lt;/p&gt;

&lt;h4 id=&quot;version-control&quot;&gt;Version Control&lt;/h4&gt;

&lt;p&gt;Just a reminder - if you followed my advice and created your source directory as a git repository, don’t forget to use git!
Do small deltas of commit + push (e.g., push it to the remote source repository once you’re done editing something on some page).&lt;/p&gt;

&lt;p&gt;You don’t want to work on something, delete it by mistake, and then find out that there’s no way to restore it :wink:.&lt;/p&gt;

&lt;h2 id=&quot;publish-your-homepage&quot;&gt;Publish Your Homepage&lt;/h2&gt;

&lt;p&gt;Once you’re satisfied with your website, i.e., you added content and even checked all the links and such, you probably want to publish it, right? :smile:.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Note that you can view it as if you’re using another device type. See instructions for Chrome &lt;a href=&quot;https://www.browserstack.com/guide/view-mobile-version-of-website-on-chrome&quot;&gt;here&lt;/a&gt;, for Safari &lt;a href=&quot;https://www.browserstack.com/guide/use-safari-devtools-to-view-mobile-web-pages&quot;&gt;here&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You should build your site with&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;JEKYLL_ENV=production bundle exec jekyll build
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;as instructed &lt;a href=&quot;https://jekyllrb.com/docs/step-by-step/01-setup/#build&quot;&gt;here&lt;/a&gt;. Note that if you’re running “bundle exec jekyll serve”, the site created isn’t the same as the one created with “build”.
Also, we need JEKYLL_ENV=production - more details &lt;a href=&quot;https://jekyllrb.com/docs/configuration/environments/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Then, copy the generated site to your local /Repos/&amp;lt; your GitHub username &amp;gt;.github.io directory, and push it to GitHub. Good luck :crossed_fingers:.&lt;/p&gt;</content><author><name>Kfir Lev-Ari</name><email>kfirlevari@gmail.com</email></author><summary type="html">In this post, I’ll share how I built this website, i.e., one hosted in GitHub Pages and powered by Jekyll &amp;amp; Minimal Mistakes.</summary></entry></feed>