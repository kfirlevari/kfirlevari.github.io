<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Distributed DNN Training - Performance Modeling - Kfir Lev-Ari</title>
<meta name="description" content="">


  <meta name="author" content="Kfir Lev-Ari">
  
  <meta property="article:author" content="Kfir Lev-Ari">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Kfir Lev-Ari">
<meta property="og:title" content="Distributed DNN Training - Performance Modeling">
<meta property="og:url" content="https://kfirlevari.github.io/DNN-Performance-Model/">


  <meta property="og:description" content="">



  <meta property="og:image" content="https://kfirlevari.github.io/assets/images/DNN.jpg">





  <meta property="article:published_time" content="2022-01-22T00:00:00-08:00">





  

  


<link rel="canonical" href="https://kfirlevari.github.io/DNN-Performance-Model/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "",
      "url": "https://kfirlevari.github.io/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Kfir Lev-Ari Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

<!-- insert favicons. use https://realfavicongenerator.net/ -->
<link rel="apple-touch-icon" sizes="180x180" href="/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
<link rel="manifest" href="/site.webmanifest">
<link rel="mask-icon" href="/safari-pinned-tab.svg" color="#5bbad5">
<meta name="msapplication-TileColor" content="#da532c">
<meta name="theme-color" content="#ffffff">

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
           
          <span class="site-subtitle">kfirlevari.github.io</span>
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/blog/">Blog</a>
            </li><li class="masthead__menu-item">
              <a href="/projects/">Projects</a>
            </li><li class="masthead__menu-item">
              <a href="/publications/">Publications</a>
            </li><li class="masthead__menu-item">
              <a href="/past-teaching/">Past Teaching</a>
            </li><li class="masthead__menu-item">
              <a href="/personal/">Personal</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(0, 0, 0, 0.6), rgba(0, 0, 0, 0.6)), url('/assets/images/DNN.jpg');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          Distributed DNN Training - Performance Modeling

        
      </h1>
      
        <p class="page__lead">
</p>
      
      

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          17 minute read
        
      </span>
    
  </p>


      
      
    </div>
  
  
    <span class="page__hero-caption"><a href="/personal/#photography">See more photos here</a>
</span>
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <img src="/assets/images/bio-photo.jpeg" alt="Kfir Lev-Ari" itemprop="image" class="u-photo">
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      Kfir Lev-Ari
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>Software Engineering @ <i class="fab fa-apple"></i></p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">Follow</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Sunnyvale, CA</span>
        </li>
      

      

      

      
        <li>
          <a href="mailto:kfirlevari@gmail.com" rel="me" class="u-email">
            <meta itemprop="email" content="kfirlevari@gmail.com" />
            <i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span>
          </a>
        </li>
      

      

      

      

      
        <li>
          <a href="https://www.linkedin.com/in/kfirlevari" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span class="label">LinkedIn</span>
          </a>
        </li>
      

      

      

      

      

      
        <li>
          <a href="https://github.com/kfirlevari" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span>
          </a>
        </li>
      

      

      

      

      

      

      

      

      

      

      

      

      

      

        
      
        <li>
          <a href="https://www.strava.com/athletes/42969031" itemprop="sameAs" rel="nofollow noopener noreferrer me">
            <i class="fab fa-fw fa-strava" aria-hidden="true"></i><span class="label">Strava</span>
          </a>
        </li>
        
        
      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Distributed DNN Training - Performance Modeling">
    <meta itemprop="description" content="">
    <meta itemprop="datePublished" content="2022-01-22T00:00:00-08:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-balance-scale-right"></i> Performance Modeling</h4></header>
              <ul class="toc__menu"><li><a href="#intro">Intro</a></li><li><a href="#paleo-model">Paleo Model</a><ul><li><a href="#computation-modeling">Computation Modeling</a></li><li><a href="#communication-modeling">Communication Modeling</a></li><li><a href="#platform-percent-of-peak">Platform Percent of Peak</a></li><li><a href="#discussion">Discussion</a></li></ul></li><li><a href="#feng-yan-et-al-model">Feng Yan et al. Model</a><ul><li><a href="#model--parallelism">Model  Parallelism</a><ul><li><a href="#feed-forward-evaluation">Feed-forward Evaluation</a></li><li><a href="#back-propagation">Back-propagation</a></li><li><a href="#weight-updates">Weight Updates</a></li></ul></li><li><a href="#data-parallelism">Data Parallelism</a><ul><li><a href="#chip-level-multiprocessing">Chip-level Multiprocessing</a></li><li><a href="#layer-replication">Layer Replication</a></li><li><a href="#multiple-model-replicas">Multiple Model Replicas</a></li></ul></li><li><a href="#epoch-time">Epoch Time</a></li><li><a href="#discussion-1">Discussion</a></li></ul></li><li><a href="#shaohuai-shi-et-al-model">Shaohuai Shi et al. Model</a><ul><li><a href="#workflow-of-sgd-and-s-sgd">Workflow of SGD and S-SGD</a></li><li><a href="#performance-model">Performance Model</a><ul><li><a href="#hiding-io-duration">Hiding I/O Duration</a></li><li><a href="#hiding-communication-duration">Hiding Communication Duration</a></li></ul></li><li><a href="#discussion-2">Discussion</a></li></ul></li><li><a href="#daniel-justus-et-al-model">Daniel Justus et al. Model</a><ul><li><a href="#layer-features">Layer Features</a></li><li><a href="#layer-specific-features">Layer Specific Features</a></li><li><a href="#hardware-features">Hardware Features</a></li><li><a href="#discussion-3">Discussion</a></li></ul></li><li><a href="#conclusions">Conclusions</a></li><li><a href="#refrences">Refrences</a></li></ul>

            </nav>
          </aside>
        
        <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<p>After looking into distributed training, I wondered what the main approaches to evaluate distributed algorithms’ performance are. Here I’ll write a summary of what I found.</p>

<h2 id="intro">Intro</h2>

<p>This post covers state-of-the-art performance models for distributed training algorithms. See <a href="/DNN-Training/">Distributed DNN Training for Practitioners</a> for a summary on distributed DNN training.</p>

<p>We focus here on performance models for predicting the execution time of a single epoch, i.e., a single forwards and backward pass through all of the training data. Predicting the required number of epochs to reach a desired level of accuracy is a different question that the following models don’t address.</p>

<p>The structure of this post is as follows -</p>

<ol>
  <li>Sections <a href="#paleo-model">Paleo Model</a>, <a href="#feng-yan-et-al-model">Feng Yan et al. Model</a>, and <a href="#shaohuai-shi-et-al-model">Shaohuai Shi et al. Model</a> give a short overview of analytical performance models for DNN algorithms.</li>
  <li>Section <a href="#daniel-justus-et-al-model">Daniel Justus et al. Model</a> describes a performance prediction tool for DNN algorithms.</li>
</ol>

<h2 id="paleo-model">Paleo Model</h2>

<p>Paleo <a class="citation" href="#Qi2017PaleoAP">[1]</a> models the expected scalability and performance of a DNN system by extracting resource requirements from a given architecture and mapping them to a point in a design space of software, hardware and communication strategies. <a href="#Figure1">Figure 1.</a> illustrates the different levels of execution time construction.</p>

<p>Generally, Paleo estimation for the epoch time is a linear calculation primarily based on the number of floating-point operations performed within an epoch.</p>

<figure class="" id="Figure1">
  <img src="/assets/images/Performance-Modeling-Images/Figure1.png" alt="" /><figcaption>
      Figure 1. The execution time is decomposed to computation and communication time; both are estimated for each pass of a neural network’s evaluation given user-specified choices within the design space of algorithms, hardware, and communications strategies.
Figure adapted from <a class="citation" href="#Qi2017PaleoAP">[1]</a>.</figcaption></figure>

<p>We now give an overview of the calculation components:</p>

<h3 id="computation-modeling">Computation Modeling</h3>

<p>For a single machine - DNN computation is expressed as a direct graph in which nodes are associated with operations on devices, and edges represent operation execution dependencies (dependencies in the sense that an operation can be executed only after the execution of all operations pointing to it is over).</p>

<p>Each layer in the DNN is modeled as a node, and the connections between layers are edges. Let us denote the immediate parents of a node (i.e., layer) \(u\) with \(Pa(u)\).</p>

<p>The overall computation time \(T(N)\) for a DNN network is built as follows:</p>

<ul>
  <li>The computation time \(T(u)\) for a single layer u that is associated with an operation \(f\) on device \(d\) consists of: 
    1. Time to fetch the input that was reproduced by the layer’s paraders, denoted \(R(Pa(u))\).
    2. Time to perform the computation of the operation, denoted \(C(f,d)\).
    3. Time to write to the local memory the outputs of the operation, denoted \(W(f,d)\).
  If we consider a sequential execution, we get: \(T(u)=R(Pa(u))+C(f,d)+W(f,d)\).
    <ul>
      <li>\(C(f,d)\)  is calculated by \(f\)’s floating-point operation counts divided by \(d\)’s floating-points operations per second (i.e., computation time = FLOP / (FLOP/sec))</li>
      <li>\(R(Pa(u))\) and \(W(f,d)\) are calculated by the required memory size divided by \(d\)’s IO bandwidth or by the communication bandwidth in case of fetching inputs from other devices.</li>
    </ul>
  </li>
  <li>The computation time \(T(N)\) for a DNN network \(N\) depends on the structure of \(N\).
    <ul>
      <li>For the sequential case, \(T(N)=\sum{T(u)}\) .</li>
      <li>For the parallel case (i.e., two or more layers that can be processed concurrently), \(N\) is reduced to a sequential network at which subgraphs of parallel layers are represented by a single super-node \(U\) (i.e., the synchronization barriers in \(N\) are the boundaries of the squashed subgraphs). The execution time of \(U\) is within the range of the best-case (completely parallel execution of the corresponding sub-graph) and worst-case (sequential execution of that sub-graph), i.e., \(T(U) \in [parallel\ execution, \ sequential\ execution]\). The parallel execution time is equal to the \(max(T(u))\) for layer \(u\) in the subgraph represented by \(U\).</li>
    </ul>
  </li>
</ul>

<p>Note that for most types of layer operations, it’s relatively easy to reason about its FLOP counts. For convolutional layers, the exact counts number is derived from the underlying algorithm used (two main algorithms are matrix multiplication and FFT, each with different complexity and IO overhead. For more info, see Section 3.1.3 in <a class="citation" href="#Qi2017PaleoAP">[1]</a>).</p>

<p>Generally, Paleo uses heuristics that are based on offline benchmarks to estimate what underlying algorithm was selected since the selection depends on filter size, strides, input size of convolutional layers, and available memory.</p>

<h3 id="communication-modeling">Communication Modeling</h3>

<p>For communication between two workers, the communication time \(T_{comm}\)​ is equal to the size of the data \(∣D∣\) to be communicated between them, divided by the bandwidth of the communication channel \(B\) , i.e., \(T_{comm} = \frac{|D∣}{B}\).<br />
The overall communication time \(T\) among \(K\) workers depends on the communication scheme used (Paleo consider these three schemes):</p>

<ul>
  <li>OneToAll - \(2KT_{comm}\)​</li>
  <li>Tree AllReduce -  \(2log2​(K)T_{comm}\)​</li>
  <li>Butterfly AllReduce - \(log2​(K)T_{comm}\)​</li>
</ul>

<h3 id="platform-percent-of-peak">Platform Percent of Peak</h3>

<p>In a perfect scenario, all devices operate at their peak FLOPS, and network and IO links are fully saturated. Since it’s unreasonable in practice, Paleo suggests a scaling constant called <em>platform percent of peak (PPP)</em> to capture the inefficiency of a platform compared to peak FLOPS (100% is the perfect scenario). PPP is calculated by running a small benchmark (on a single GPU for computation PPP and for communication PPP).</p>

<h3 id="discussion">Discussion</h3>

<p>Pros:</p>

<ul>
  <li>Model results seem very close to the actual execution time</li>
  <li>Friendly open-source graphical interface  <a class="citation" href="#Qi2017PaleoAPGitHub">[2]</a></li>
</ul>

<p>Cons:</p>

<ul>
  <li>The paper is from 2017, so it doesn’t consider newer algorithms, e.g.:
    <ul>
      <li>Consider only data and model parallelism - pipelining isn’t in the paper or hybrid approaches.</li>
      <li>Do not consider the recently introduced communication schemes, such as a butterfly mixing scheme or non-deterministic asynchronous parameter servers.</li>
    </ul>
  </li>
  <li>A linear model does not necessarily capture the complexity of the problem (see <a class="citation" href="#Justus2018PredictingTC">[3]</a> for more information about the flaws of a linear FLOPs-based estimation).</li>
</ul>

<h2 id="feng-yan-et-al-model">Feng Yan et al. Model</h2>

<p>In <a class="citation" href="#10.1145/2783258.2783270">[4]</a>, they suggest a performance model for predicting the efficiency of a given configuration, i.e., predicting the training epoch duration when using that configuration. Later, they use this model for reducing the search time for efficient and accurate configuration - by empirically evaluating efficient configurations (the idea is that searching a good configuration this way is faster since we iterate over efficient configurations).</p>

<p>Next, we give intuition to their performance model (see Section 4. in <a class="citation" href="#10.1145/2783258.2783270">[4]</a> for the full description and equations). Generally, the performance prediction calculation is based on multiple small benchmark results.</p>

<p>This model integrates Model Parallelism and Data Parallelism - we’ll start by showing a model for the training time of a single sample across multiple machines (as done in Model Parallelism), and then we’ll extend this model to consider different samples on different machines (i.e., to capture Data Parallelism).</p>

<h3 id="model--parallelism">Model  Parallelism</h3>

<p>Each layer is partitioned into segments that are processed in parallel, meaning that the layer’s execution time is determined by the slowest segment. If we assume that the layer is partitioned into equal segments and also that each segment is processed by the same processing power, then we can estimate the layer’s training time by using any of its segments. The total training time of a segment is equal to the sum of its feed-forward evaluation, back-propagation, and weight updates (each of these consists of computation and communication):</p>

<h4 id="feed-forward-evaluation">Feed-forward Evaluation</h4>

<p>For each segment, the feed-forward evaluation time is equal to the time spent in computing the output activations of the neurons in the segment and communicating activations from the connected segments in the previous layer.</p>

<ul>
  <li>Computation: they suggest evaluating this duration for a given machine by running a micro-benchmark that consists of a canonical feed-forward evaluation. Generally, this benchmark iterates over the neurons of a segment, and for each neuron: (1) accumulates activations (each action is a multiply-add, based on the previous layer activations and the corresponding weight), and (2) computes an activation function for the neuron using the sum that was calculated in (1). See <a href="#alg1">Algorithm 1.</a> for the pseudo-code.</li>
</ul>

<figure class="" id="alg1">
  <img src="/assets/images/Performance-Modeling-Images/Alg1.png" alt="" /><figcaption>
      Algorithm 1. Canonical feed-forward evaluation.
Algorithm adapted from <a class="citation" href="#10.1145/2783258.2783270">[4]</a>.</figcaption></figure>

<ul>
  <li>Communication: given that activations can be sent asynchronously, they assume that the communication time of feed-forward evaluation is dominated by the delay in receiving cross-machine activations from the previous layer. 
  Thus, the communication time is equal to the sum of (1) network latency of sending one bit of data between two workers and (2) the transfer time of the data size received by segment (i.e., the size of each activation multiplied by the number of remote activations and divided by the bandwidth of the machine’s NIC).</li>
</ul>

<h4 id="back-propagation">Back-propagation</h4>

<ul>
  <li>Computation: since the calculation structure is similar to the feed-forward calculation, the cost estimation is also similar.</li>
  <li>Communication: here, we follow similar reasoning to the one we showed in the communication of forward-feed evaluation, only this time, the data that is received by the segment consists of errors from the following layer, instead of activations from the previous layer.</li>
</ul>

<h4 id="weight-updates">Weight Updates</h4>

<ul>
  <li>Computation: when updating the segment’s weights, for each weight between two connected neurons (i.e., one that belongs to the segment and another that belongs to another segment), a weight delta is computed using an error term and an activation value (a multiplication action). Then this delta is added to the weight. This means that the total computation cost of weight updates is these multiplications and add actions, multiplied by the number of connections between the segment’s neurons to other segments’ neurons.</li>
  <li>Communication: at this point, when we only consider model parallelism, weights aren’t communicated; therefore, the communication cost is 0.</li>
</ul>

<h3 id="data-parallelism">Data Parallelism</h3>

<p>We now show the required modifications to the model above for supporting concurrent processing of multiple samples, as done in data parallelism. We focus on three forms - (1) chip-level multiprocessing, (2) layer replication, and (3) model replicas with parameter servers.</p>

<h4 id="chip-level-multiprocessing">Chip-level Multiprocessing</h4>

<p>Different cores of a chip-level multiprocessing system can process different samples concurrently (e.g., a 16 core processing 16 samples at a time) while asynchronously sharing weights through shared memory. The new dimension here is the number of concurrent threads: a higher value increases concurrency but also increases the potential interference among the threads. Now each segment will be identified by its layer, partition number, and thread ID.</p>

<ul>
  <li>Computation: Concurrent training of multiple samples may interfere with each other, competing for memory bandwidth and thus affecting per-sample computation time. For a given number of threads, a performance interference factor is defined to model the interference among them. We estimate it by running a multi-threaded version of the canonical form such that each thread processes the same code segment using different cores. The interference factor then is estimated as the ratio of the multi-thread execution time and the single-thread execution time. The new estimation for computation time of the segment is received by multiplying this interference factor by the estimated computation time of the segment we defined in the model parallelism model (i.e., in a single thread execution).
    <ul>
      <li>Data parallelism degree - is defined as the training concurrency degree for a given layer. When we only consider threads concurrency, the degree is equal to the number of threads (this component will later be used for calculating the overall training time).</li>
    </ul>
  </li>
  <li>Communication: when training multiple samples with multiple threads, the network bandwidth is shared among threads, i.e., each thread gets an equal portion of the bandwidth. This affects the data transfer calculation as we now have less bandwidth (a function of the number of threads). In addition, the latency is also a function of the threads number since it might increase due to resource allocation on each side (for supporting multiple connections).</li>
</ul>

<h4 id="layer-replication">Layer Replication</h4>

<p>In layer replication, multiple machines process a different subset of training data on the same part of the network, thus reducing the network overheads of cross-machine activations. The replication factor doesn’t affect the computation time of each segment or the communication time.</p>

<ul>
  <li>Data parallelism degree - when adding replication, the data parallelism degree is equal to the number of threads multiplied by the replication number.</li>
</ul>

<h4 id="multiple-model-replicas">Multiple Model Replicas</h4>

<p>We now add to the performance model the scenario in which model replicas are trained in parallel using a sharded parameter server scheme. The weight communication load scales with the number of model replicas (since each replica induces more communication load on the parameter servers), and the optimal ratio between computation and communication depends on both the DNN and hardware.</p>

<ul>
  <li>Computation: same as before, since a single sample computation isn’t affected by model replication.
    <ul>
      <li>Data parallelism degree - extended by a multiplication factor equals the number of model replicas.</li>
    </ul>
  </li>
  <li>Communication: while writing weights to parameter servers can be done asynchronously and does not affect training time, reading weights is a blocking operation since a replica must wait for the weights in order to train the model.  Therefore we add the reading time to the model. The worst-case for the reading time is when all replicas read from the same parameter server at the same time. In this case, the data size (that is read from the parameter server) is equal to the number of replicas multiplied by the size of the model weights, and the transfer time is that data size divided by the bandwidth of a single thread (assuming multiple threads on the parameter server). In addition, we consider the latency between the replica and the parameter server to be also a function of the thread numbers. The actual time is lower than the worst-case (in the paper, they also state the best case as the another bound).</li>
</ul>

<h3 id="epoch-time">Epoch Time</h3>

<p>In conclusion, the total estimated epoch time in this performance model is equal to the sum of the following components:</p>

<ol>
  <li>The time it takes to read the model from the parameter server multiplied by the total number of times a replica reads from the parameter servers (the latter number is equal to the total number of samples divided by the number of samples used by a replica before reading from the parameter server).</li>
  <li>The total time it takes to train all layers. For a given layer, the time is equal to the total number of samples divided by the data parallelism degree (i.e., this gives us a factor for sequential segments - higher number means that we have more sequential segments), multiplied by the time it takes to process a single segment. Processing a segment is the sum of (1) feed-forward propagation computation and communication, (2) back-propagation computation and communication, and (3) update weights computation.</li>
</ol>

<h3 id="discussion-1">Discussion</h3>

<p>Pros:</p>

<ul>
  <li>Their optimizer, which is using their model, was able to find an optimal configuration (for benchmarks) efficiently.</li>
  <li>The model takes into account also custom hardware (e.g., FPGAs, ASICs, and RDMA)</li>
</ul>

<p>Cons:</p>

<ul>
  <li>It seems like multiple benchmark executions are required in order to get the right estimators, e.g., at least one benchmark execution for every tested number of threads.</li>
  <li>It doesn’t consider the decentralized approach of model parallelism.</li>
</ul>

<h2 id="shaohuai-shi-et-al-model">Shaohuai Shi et al. Model</h2>

<p>In <a class="citation" href="#Shi2018PerformanceMA">[5]</a>, the authors analyze frameworks’ training time while running on multiple GPUs and nodes. To that end, they build a high-level performance model for training DNNs with synchronous SGD (S-SGD) on GPUs. Generally, their model divides the iteration time into its main components and later measures them as they evaluate the performance.</p>

<h3 id="workflow-of-sgd-and-s-sgd">Workflow of SGD and S-SGD</h3>

<p>When using a single GPU, the main components of a mini-batch SGD iteration duration are:</p>

<ol>
  <li>Read duration of data from disk to CPU memory</li>
  <li>Data transfer duration from CPU to GPU</li>
  <li>GPU forward feed duration</li>
  <li>GPU backward propagation duration</li>
  <li>Duration of updating the model using the gradients</li>
</ol>

<p>When extended to S-SGD (i.e., for using multiple GPUs and nodes), we have the following phases:</p>

<ol>
  <li>Each node reads the same amount of samples.</li>
  <li>Each node distributed the samples evenly to all of its GPUs.</li>
  <li>Each GPU runs forward feeds and back propagations.</li>
  <li>The gradients are averaged among all node’s GPUs - there is synchronization among GPUs as they must wait for all to complete this step.</li>
  <li>Each GPU updates its parameters.</li>
</ol>

<h3 id="performance-model">Performance Model</h3>

<p>The iteration time titer​ can be written as a sum of three components:</p>

<ul>
  <li>\(t_{io}\)​ - I/O duration in that iteration - reading data from disk to CPU memory.</li>
  <li>\(t_{gpu}\)​ - a duration that sums the work of the GPU. It includes the data transfer time from CPU to GPU, the forward feeds, the back propagations, and the model update operations.</li>
  <li>\(t_{comm}\)​ - duration of gradients communication (equals 0 if we only use one GPU).</li>
</ul>

<p>Using pipelining, we can partially hide tio​ and \(t_{comm}\)​, as we now explain.</p>

<h4 id="hiding-io-duration">Hiding I/O Duration</h4>

<p>Reducing the I/O duration in step 1 is done by reading new data into CPU memory in parallel with a previous iteration’s computation and communication steps.
By doing so, the average iteration time becomes \(t_{iter}​=max(t_{gpu}​+t_{comm}​,t_{io}​)\)</p>

<h4 id="hiding-communication-duration">Hiding Communication Duration</h4>

<p>In each layer, the gradient computation has no dependency on updating the following layers, which means that this computation can be parallelized with the communication (i.e., gradient aggregation in phase 4) in the next layer. The exact level of parallelism depends on the duration of the computation and the duration of the communication.</p>

<h3 id="discussion-2">Discussion</h3>

<p>Pros:</p>

<ul>
  <li>A simple model that can express the expected speedup for a given number of GPUs.</li>
  <li>The effect of I/O and communication on scalability is visible when examining the model equations.</li>
</ul>

<p>Cons:</p>

<ul>
  <li>Mostly a guideline for different time components that need to be measured, most of the work is measuring the different durations.</li>
</ul>

<h2 id="daniel-justus-et-al-model">Daniel Justus et al. Model</h2>

<p>In <a class="citation" href="#Justus2018PredictingTC">[3]</a>, the authors propose to use a deep neural network to predict the training execution time of a single layer and then build the total expected execution time by summing the results of all layers. They suggest training that predictor DNN using features that are derived from the computational resource used, the network that is trained, and the data used for training. They categorized these features into layer features, layer-specific features, and hardware features. Due to the size of the resulting feature space, they trained their predictor DNN via random subsamples. We now give a summary of the features they mentioned in the paper:</p>

<h3 id="layer-features">Layer Features</h3>

<p>This category mainly covers the layer’s hyper-parameters, such as:</p>

<ul>
  <li>Activation function type, e.g., ReLU, softmax, sigmoid, etc…</li>
  <li>Optimizer type, e.g., Gradient Descent, Adadelta, Adagrad, Momentum, Adam, and RMS Prop.</li>
</ul>

<h3 id="layer-specific-features">Layer Specific Features</h3>

<p>This category covers features that correspond to specific layer types. Here is a list of a layer type and the features that can be used for it:</p>

<ul>
  <li>Fully connected layer - number of inputs to that layer, number of neurons in that layer.</li>
  <li>Convolutional layer - matrix size, kernel size, input number of channels, output number of channels, stride size, input padding size</li>
  <li>Pooling layer - kernel size, stride size, padding size</li>
  <li>Recurrent features - same as a fully connected layer, and also type of recurrence (e.g., LSTM, GRU) and a binary value indicating if the RNN is bidirectional</li>
</ul>

<h3 id="hardware-features">Hardware Features</h3>

<p>The features in this category describe hardware characteristics. For example, for GPU, we can have the following features:</p>

<ul>
  <li>Type, e.g., NVIDIA w/ Turing microarchitecture, or Volta, etc…</li>
  <li>Count (number of GPUs used)</li>
  <li>Memory size</li>
  <li>Clock speed</li>
  <li>Memory bandwidth</li>
  <li>Core count</li>
  <li>GFLOPS peak performance</li>
  <li>Connectivity type</li>
</ul>

<p>See Section 5. in <a class="citation" href="#Justus2018PredictingTC">[3]</a> for details about the methodology they used while training their predictor model.</p>

<h3 id="discussion-3">Discussion</h3>

<p>Pros:</p>

<ul>
  <li>Open source code <a class="citation" href="#Justus2018PredictingTCGitHub">[6]</a></li>
  <li>Their trained model accurately predicted the required execution time for a wide range of the most frequently used components of neural networks.</li>
  <li>It can be extended as much as needed.</li>
</ul>

<p>Cons:</p>

<ul>
  <li>It probably requires a long preparation phase before the approach is usable, compared to the previous approaches.</li>
  <li>Predictions are less explainable compared to other models.</li>
</ul>

<h2 id="conclusions">Conclusions</h2>

<p>In this post, we saw four performance models, from a model that requires the least amount of benchmarking (Paleo - that bases the training time expectation on the cost of FLOP operations) to a model that requires the most (the last model that requires a dataset of whole training sessions and corresponding durations).</p>

<h2 id="refrences">Refrences</h2>

<ol class="bibliography"><li><span id="Qi2017PaleoAP">Hang Qi, Evan R. Sparks, and Ameet Talwalkar. <b>Paleo: A Performance Model for Deep Neural Networks</b>. In: <i>5th International Conference on Learning Representations, ICLR 2017,
               Toulon, France, April 24-26, 2017, Conference Track Proceedings</i>, OpenReview.net, 2017.</span></li>
<li><span id="Qi2017PaleoAPGitHub">Hang Qi, Evan R. Sparks, and Ameet S. Talwalkar. <b>Paleo - An analytical model to estimate the scalability and performance of deep learning systems <a href="https://talwalkarlab.github.io/paleo/">link</a></b>, 2107.</span></li>
<li><span id="Justus2018PredictingTC">Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. <b>Predicting the Computational Cost of Deep Learning Models</b>. <i>2018 IEEE International Conference on Big Data (Big Data)</i>, :3873–3882, 2018.</span></li>
<li><span id="10.1145/2783258.2783270">Feng Yan, Olatunji Ruwase, Yuxiong He, and Trishul Chilimbi. <b>Performance Modeling and Scalability Optimization of Distributed Deep Learning Systems</b>. In: <i>Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</i>, pp. 1355–1364, Association for Computing Machinery, New York, NY, USA, 2015.</span></li>
<li><span id="Shi2018PerformanceMA">Shaohuai Shi and Xiaowen Chu. <b>Performance Modeling and Evaluation of Distributed Deep Learning Frameworks on GPUs</b>. <i>2018 IEEE 16th Intl Conf on Dependable, Autonomic and Secure Computing, 16th Intl Conf on Pervasive Intelligence and Computing, 4th Intl Conf on Big Data Intelligence and Computing and Cyber Science and Technology Congress(DASC/PiCom/DataCom/CyberSciTech)</i>, :949–957, 2018.</span></li>
<li><span id="Justus2018PredictingTCGitHub">Daniel Justus, John Brennan, Stephen Bonner, and Andrew Stephen McGough. <b>performance-prediction <a href="https://github.com/CDECatapult/ml-performance-prediction">link</a></b>, 2018.</span></li></ol>

        
      </section>

      <footer class="page__meta">
        
        


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time class="dt-published" datetime="2022-01-22T00:00:00-08:00">January 22, 2022</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">Share on</h4>
  

  <a href="https://twitter.com/intent/tweet?text=Distributed+DNN+Training+-+Performance+Modeling%20https%3A%2F%2Fkfirlevari.github.io%2FDNN-Performance-Model%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=https%3A%2F%2Fkfirlevari.github.io%2FDNN-Performance-Model%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=https%3A%2F%2Fkfirlevari.github.io%2FDNN-Performance-Model%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="Share on LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/ML-World/" class="pagination--pager" title="Machine Learning Knowledge Graph
">Previous</a>
    
    
      <a href="#" class="pagination--pager disabled">Next</a>
    
  </nav>

    </div>

    
      <div class="page__comments">
  
  
      <h4 class="page__comments-title">Leave a comment</h4>
      <section id="giscus-comments"></section>
    
</div>

    
  </article>

  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="http://il.linkedin.com/in/kfirlevari/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i> Linkedin</a></li>
        
      
        
          <li><a href="https://scholar.google.co.il/citations?hl=en&user=SdzwxKEAAAAJ&view_op=list_works&sortby=pubdate" rel="nofollow noopener noreferrer"><i class="ai ai-google-scholar" aria-hidden="true"></i> Google Scholar</a></li>
        
      
        
          <li><a href="http://dblp2.uni-trier.de/pers/hd/l/Lev=Ari:Kfir" rel="nofollow noopener noreferrer"><i class="ai ai-dblp" aria-hidden="true"></i> DBLP</a></li>
        
      
        
          <li><a href="https://github.com/kfirlevari" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
          <li><a href="https://www.patreon.com/user/creators?u=65820883" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-patreon" aria-hidden="true"></i> Patreon</a></li>
        
      
        
          <li><a href="https://bandcamp.com/kfirlev-ari" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-bandcamp" aria-hidden="true"></i> Bandcamp</a></li>
        
      
        
          <li><a href="https://www.strava.com/athletes/42969031" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-strava" aria-hidden="true"></i> Strava</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 Kfir Lev-Ari. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>







  <!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-CYB6SXB214"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-CYB6SXB214', { 'anonymize_ip': false});
</script>






    <script>
  'use strict';

  (function () {
    var commentContainer = document.querySelector('#giscus-comments');

    if (!commentContainer) {
      return;
    }

    var script = document.createElement('script');
    script.setAttribute('src', 'https://giscus.app/client.js');
    script.setAttribute('data-repo', 'kfirlevari/kfirlevari.github.io');
    script.setAttribute('data-repo-id', 'MDEwOlJlcG9zaXRvcnk2Mzk1MzM2MA==');
    script.setAttribute('data-category', 'Announcements');
    script.setAttribute('data-category-id', 'DIC_kwDOA8_Z0M4CAQqJ');
    script.setAttribute('data-mapping', 'pathname');
    script.setAttribute('data-reactions-enabled', '1');
    script.setAttribute('data-theme', 'light');
    script.setAttribute('crossorigin', 'anonymous');

    commentContainer.appendChild(script);
  })();
</script>
  





  </body>
</html>
